{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/wbb/result/qq_prediction/\"\n",
    "\n",
    "train_positive_pairs = sc.textFile(base_path + \"new_positive_pairs_features/day*/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "train_unlabled_pairs = sc.textFile(base_path + \"new_unlabled_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "\n",
    "test_positive_pairs = sc.textFile(base_path + \"test_positive_pairs/part-*\")\n",
    "\n",
    "test_pairs_features = sc.textFile(base_path + \"new_test_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "\n",
    "test_pairs_features_50w = sc.textFile(base_path + \"test_pairs_features_50w/part-*\").map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"test_positive_pairs_list = test_positive_pairs.collect()   # 第四天数据中的正例对\n",
    "test_data_4thday = test_pairs_features_50w.map(lambda x: Row(label=1.0 if x[0] in test_positive_pairs_list else 0.0,\\\n",
    "                                                         features=Vectors.dense(x[1]))).toDF()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_positive_pairs_list = test_positive_pairs.collect()   # 第四天数据中的正例对\\n#test_pairs_features = sc.textFile(base_path + \"new_test_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\\ntest_pos_features = test_pairs_features.filter(lambda x: x[0] in test_positive_pairs_list)\\ntest_neg_features = test_pairs_features.filter(lambda x: x[0] not in test_positive_pairs_list).takeSample(False, 80000)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_positive_pairs_list = test_positive_pairs.collect()   # 第四天数据中的正例对\n",
    "#test_pairs_features = sc.textFile(base_path + \"new_test_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "test_pos_features = test_pairs_features.filter(lambda x: x[0] in test_positive_pairs_list)\n",
    "test_neg_features = test_pairs_features.filter(lambda x: x[0] not in test_positive_pairs_list).takeSample(False, 80000)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"test_pos_features.saveAsTextFile(base_path + \"test_pos_features\")\n",
    "sc.parallelize(test_neg_features).saveAsTextFile(base_path + \"test_neg_features_8w\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"test_data_4thday = test_pos_features.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(sc.parallelize(test_neg_features).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pos_features = sc.textFile(base_path + \"test_pos_features/part-*\").map(lambda x: eval(x))\n",
    "test_neg_features = sc.textFile(base_path + \"test_neg_features_8w/part-*\").map(lambda x: eval(x))\n",
    "test_data_4thday = test_pos_features.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(test_neg_features.map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_4thday.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_4thday.first().features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_label = [i.label for i in test_data_4thday.select('label').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157653\n"
     ]
    }
   ],
   "source": [
    "test_data_4thday_length = len(y_label) #test_data_4thday.count()\n",
    "print(test_data_4thday_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 77653 1.0302242025420782\n"
     ]
    }
   ],
   "source": [
    "num1 = num0 = 0\n",
    "for i in y_label:\n",
    "    if i == 1.0:\n",
    "        num1 += 1\n",
    "    if i == 0.0:\n",
    "        num0 += 1\n",
    "print(num0, num1, num0/num1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 28 12:49:06 2017\n",
      "Tue Feb 28 12:52:12 2017\n",
      "Tue Feb 28 13:05:21 2017\n",
      "95740 0.999999999936 0.500002544444 0.824591069664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for alfa in [0.35]: #0.30\n",
    "    with open('qq_PU_alfa_1_v331.log', 'a+') as log_file1:\n",
    "    # \n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        log_file1.write(\"\\n===GET RN===\\n\" + '\\n')\n",
    "        log_file1.write( \"==========alfa: \" + str(alfa) + \"============\\n\")\n",
    "        print(time.asctime())\n",
    "        # train_unlabeled -> train_neg\n",
    "        #train_negative_pairs = train_unlabled_pairs.takeSample(False, int(alfa * 8268837))\n",
    "        #sc.parallelize(train_negative_pairs).coalesce(20).saveAsTextFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa))\n",
    "        train_negative_pairs = sc.textFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa)).map(lambda x: eval(x))#.collect()\n",
    "\n",
    "        #log_file1.write(\"length of train_negative_pairs: \" + str(len(train_negative_pairs)) + '\\n')\n",
    "        #print(time.asctime())\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "        # ===============================================\n",
    "        # with PU\n",
    "        # train_positive_pairs.count() = 235586\n",
    "        blta = 0.8  # 从train pos 里面取一部分加到 neg 里面组成PU数据\n",
    "        pnsplit = int(235586*blta)   # pnsplit以前的部分作为pu_pos, 以后的部分作为pu_neg\n",
    "        pu_pos_features = train_positive_pairs.collect()[:pnsplit]\n",
    "        pu_neg_features = train_negative_pairs.collect() + train_positive_pairs.collect()[pnsplit:]\n",
    "        #pu_spy = [Vectors.dense(x[1]) for x in train_positive_pairs.collect()[pnsplit:]]\n",
    "        \n",
    "        log_file1.write(\"length of pu_pos_features: \" + str(len(pu_pos_features)) + '\\n')\n",
    "        log_file1.write(\"length of pu_neg_features: \" + str(len(pu_neg_features)) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n\\n')\n",
    "        print(time.asctime())\n",
    "\n",
    "        gamma=0.3  # 从PU数据里面取一部分训练，另一部分作为测试\n",
    "        pu_train_pos = sc.parallelize(pu_pos_features).takeSample(False, int(len(pu_pos_features) * gamma))  # list\n",
    "        pu_test_pos = sc.parallelize(pu_pos_features).subtractByKey(sc.parallelize(pu_train_pos)).collect()\n",
    "        log_file1.write(\"length of pu_train_pos: \" + str(len(pu_train_pos)) + '\\n')\n",
    "        log_file1.write(\"length of pu_test_pos: \" + str(len(pu_test_pos)) + '\\n')\n",
    "        pu_train_neg = sc.parallelize(pu_neg_features, 50).takeSample(False, int(len(pu_neg_features) * gamma))  # list\n",
    "        pu_test_neg = sc.parallelize(pu_neg_features).subtractByKey(sc.parallelize(pu_train_neg)).collect()\n",
    "        log_file1.write(\"length of pu_train_neg: \" + str(len(pu_train_neg)) + '\\n')\n",
    "        log_file1.write(\"length of pu_test_neg: \" + str(len(pu_test_neg)) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n\\n')\n",
    "        print(time.asctime())\n",
    "\n",
    "        pu_train_data = sc.parallelize(pu_train_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(sc.parallelize(pu_train_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\n",
    "\n",
    "        pu_test_data = sc.parallelize(pu_test_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1]), pair=x[0]))\\\n",
    "        .union(sc.parallelize(pu_test_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1]), pair=x[0]))).toDF()\n",
    "\n",
    "        \n",
    "        ### LR\n",
    "        log_file1.write(\"###### LogisticRegression ######\\n\")\n",
    "        publr = LogisticRegression().fit(pu_train_data)\n",
    "        pu_test_data_preRes_LR = publr.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "        predict_to_0_LR = pu_test_data_preRes_LR.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "        predict_to_0_prob_LR = predict_to_0_LR.map(lambda x: x.probability[0]).collect()\n",
    "        \n",
    "        prob_LR_max = max(predict_to_0_prob_LR)\n",
    "        prob_LR_mean = sum(predict_to_0_prob_LR)/len(predict_to_0_prob_LR)\n",
    "        print(len(predict_to_0_prob_LR), prob_LR_max, min(predict_to_0_prob_LR), prob_LR_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_LR) ++ max(predict_to_0_prob_LR) ++ min(predict_to_0_prob_LR) ++ mean(predict_to_0_prob_LR) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_LR)) + ', ' + str(prob_LR_max) + ', ' + str(min(predict_to_0_prob_LR)) + ', ' + str(prob_LR_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        #RN_LR_max = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] >= prob_LR_max).map(lambda x: x.features).collect()\n",
    "        RN_LR_mean = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] > prob_LR_mean).map(lambda x: x.features).takeSample(False, 240000)\n",
    "        #log_file1.write('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n')\n",
    "        #log_file1.write(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "        #print('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n')\n",
    "        #print(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "        #sc.parallelize(RN_LR_max).saveAsTextFile(base_path + \"RN/RN_LR_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_LR_mean).saveAsTextFile(base_path + \"RN/RN_LR_mean-\" + str(alfa))\n",
    "        \n",
    "        \"\"\"    \n",
    "        ### NB\n",
    "        log_file1.write(\"###### NaiveBayes ######\\n\")\n",
    "        pubNB = NaiveBayes().fit(pu_train_data)\n",
    "        pu_test_data_preRes_NB = pubNB.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "        predict_to_0_NB = pu_test_data_preRes_NB.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "        predict_to_0_prob_NB = predict_to_0_NB.map(lambda x: x.probability[0]).collect()\n",
    "        \n",
    "        prob_NB_max = max(predict_to_0_prob_NB)\n",
    "        prob_NB_mean = sum(predict_to_0_prob_NB)/len(predict_to_0_prob_NB)\n",
    "        print(len(predict_to_0_prob_NB), prob_NB_max, min(predict_to_0_prob_NB), prob_NB_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_NB) ++ max(predict_to_0_prob_NB) ++ min(predict_to_0_prob_NB) ++ mean(predict_to_0_prob_NB) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_NB)) + ', ' + str(prob_NB_max) + ', ' + str(min(predict_to_0_prob_NB)) + ', ' + str(prob_NB_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        RN_NB_max = pu_test_data_preRes_NB.rdd.filter(lambda x: x.probability[0] >= prob_NB_max).map(lambda x: x.features).collect()\n",
    "        RN_NB_mean = pu_test_data_preRes_NB.rdd.filter(lambda x: x.probability[0] > prob_NB_mean).map(lambda x: x.features).collect()\n",
    "        log_file1.write('len(RN_NB_max): '+str(len(RN_NB_max))+'\\n')\n",
    "        log_file1.write(\"len(RN_NB_mean):\" + str(len(RN_NB_mean))+'\\n')\n",
    "        print('len(RN_NB_max): '+str(len(RN_NB_max))+'\\n')\n",
    "        print(\"len(RN_NB_mean):\" + str(len(RN_NB_mean))+'\\n')\n",
    "        sc.parallelize(RN_NB_max).saveAsTextFile(base_path + \"RN/RN_NB_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_NB_mean).saveAsTextFile(base_path + \"RN/RN_NB_mean-\" + str(alfa))\n",
    "              \n",
    "        ### DT\n",
    "        log_file1.write(\"###### DecisionTreeClassifier ######\\n\")\n",
    "        pubDT = DecisionTreeClassifier().fit(pu_train_data)\n",
    "        pu_test_data_preRes_DT = pubDT.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\") \n",
    "\n",
    "        predict_to_0_DT = pu_test_data_preRes_DT.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "        predict_to_0_prob_DT = predict_to_0_DT.map(lambda x: x.probability[0]).collect()\n",
    "        \n",
    "        prob_DT_max = max(predict_to_0_prob_DT)\n",
    "        prob_DT_mean = sum(predict_to_0_prob_DT)/len(predict_to_0_prob_DT)\n",
    "        print(len(predict_to_0_prob_DT), prob_DT_max, min(predict_to_0_prob_DT), prob_DT_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_DT) ++ max(predict_to_0_prob_DT) ++ min(predict_to_0_prob_DT) ++ mean(predict_to_0_prob_DT) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_DT)) + ', ' + str(prob_DT_max) + ', ' + str(min(predict_to_0_prob_DT)) + ', ' + str(prob_DT_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        #RN_DT_max = pu_test_data_preRes_DT.rdd.filter(lambda x: x.probability[0] >= prob_DT_max).map(lambda x: x.features).collect()\n",
    "        RN_DT_mean = pu_test_data_preRes_DT.rdd.filter(lambda x: x.probability[0] > prob_DT_mean).map(lambda x: x.features).takeSample(False, 240000)\n",
    "        #log_file1.write('len(RN_DT_max): '+str(len(RN_DT_max))+'\\n')\n",
    "        #log_file1.write(\"len(RN_DT_mean):\" + str(len(RN_DT_mean))+'\\n')\n",
    "        #print('len(RN_DT_max): '+str(len(RN_DT_max))+'\\n')\n",
    "        #print(\"len(RN_DT_mean):\" + str(len(RN_DT_mean))+'\\n')\n",
    "        #sc.parallelize(RN_DT_max).saveAsTextFile(base_path + \"RN/RN_DT_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_DT_mean).saveAsTextFile(base_path + \"RN/RN_DT_mean-\" + str(alfa))\n",
    "            \n",
    "        ### RandomForestClassifier\n",
    "        log_file1.write(\"###### RandomForestClassifier ######\\n\")\n",
    "        pubRF = RandomForestClassifier().fit(pu_train_data)\n",
    "        pu_test_data_preRes_RF = pubRF.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "        predict_to_0_RF = pu_test_data_preRes_RF.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "        predict_to_0_prob_RF = predict_to_0_RF.map(lambda x: x.probability[0]).collect()\n",
    "        \n",
    "        prob_RF_max = max(predict_to_0_prob_RF)\n",
    "        prob_RF_mean = sum(predict_to_0_prob_RF)/len(predict_to_0_prob_RF)\n",
    "        print(len(predict_to_0_prob_RF), prob_RF_max, min(predict_to_0_prob_RF), prob_RF_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_RF) ++ max(predict_to_0_prob_RF) ++ min(predict_to_0_prob_RF) ++ mean(predict_to_0_prob_RF) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_RF)) + ', ' + str(prob_RF_max) + ', ' + str(min(predict_to_0_prob_RF)) + ', ' + str(prob_RF_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        #RN_RF_max = pu_test_data_preRes_RF.rdd.filter(lambda x: x.probability[0] >= prob_RF_max).map(lambda x: x.features).collect()\n",
    "        RN_RF_mean = pu_test_data_preRes_RF.rdd.filter(lambda x: x.probability[0] > prob_RF_mean).map(lambda x: x.features).takeSample(False, 240000)\n",
    "        #log_file1.write('len(RN_RF_max): '+str(len(RN_RF_max))+'\\n')\n",
    "        #log_file1.write(\"len(RN_RF_mean):\" + str(len(RN_RF_mean))+'\\n')\n",
    "        #print('len(RN_RF_max): '+str(len(RN_RF_max))+'\\n')\n",
    "        #print(\"len(RN_RF_mean):\" + str(len(RN_RF_mean))+'\\n')\n",
    "        #sc.parallelize(RN_RF_max).saveAsTextFile(base_path + \"RN/RN_RF_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_RF_mean).saveAsTextFile(base_path + \"RN/RN_RF_mean-\" + str(alfa))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157653\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 28 15:57:38 2017\n",
      "240000\n",
      "157653\n",
      "0.926223069295 0.771435313298 0.841772633378 0.8284904188312306\n",
      "Tue Feb 28 16:04:37 2017\n"
     ]
    }
   ],
   "source": [
    "with open('qq_PU_alfa_2_v3321.log', 'a+') as log_file2:\n",
    "    for alfa in [0.35]: # 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, , 0.40]: #, \n",
    "        log_file2.write('\\n' + time.asctime() + '\\n')\n",
    "        log_file2.write(\"===Predict with RN(Sample)===\\n\" + '\\n')\n",
    "        log_file2.write( \"==========alfa: \" + str(alfa) + \"============\\n\")\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # LR\n",
    "        log_file2.write('###### LogisticRegression #######\\n')\n",
    "        RN_LR = sc.parallelize(sc.textFile(base_path + \"RN/RN_LR_mean-\" + str(alfa)).map(lambda x: eval(x)).takeSample(False, 240000))\n",
    "        print(RN_LR.count())\n",
    "        PU_train_data_LR = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_LR.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "        \n",
    "        lr = LogisticRegression().fit(PU_train_data_LR)\n",
    "        predict_4th_day_LR = lr.transform(test_data_4thday.select('features'))\n",
    "        \n",
    "        y_predict_PU_LR = [i.prediction for i in predict_4th_day_LR.select('prediction').collect()]\n",
    "        print(len(y_predict_PU_LR))\n",
    "        recall_score_PU_LR = recall_score(y_label, y_predict_PU_LR)\n",
    "        precision_score_PU_LR = precision_score(y_label, y_predict_PU_LR)\n",
    "        f1_score_PU_LR = f1_score(y_label, y_predict_PU_LR)\n",
    "        tp_LR = 0\n",
    "        tn_LR = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label[i] == 1.0 and y_predict_PU_LR[i] == 1.0:\n",
    "                tp_LR += 1\n",
    "            if y_label[i] == 0.0 and y_predict_PU_LR[i] == 0.0:\n",
    "                tn_LR += 1\n",
    "        accuracy_LR = (tp_LR + tn_LR)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_PU_LR, precision_score_PU_LR, f1_score_PU_LR, accuracy_LR)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_LR: \" + str(recall_score_PU_LR) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_LR: \" + str(precision_score_PU_LR) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_LR: \" + str(f1_score_PU_LR) + '\\n')\n",
    "        log_file2.write(\"accuracy_LR: \" + str(accuracy_LR) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        \"\"\"# NB\n",
    "        log_file2.write('###### NaiveBayes #######\\n')\n",
    "        RN_NB = sc.parallelize(sc.textFile(base_path + \"RN/RN_NB_mean-\" + str(alfa)).map(lambda x: eval(x)).takeSample(False, 240000))\n",
    "        \n",
    "        PU_train_data_NB = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_NB.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "        \n",
    "        NB = NaiveBayes().fit(PU_train_data_NB)\n",
    "        predict_4th_day_NB = NB.transform(test_data_4thday.select('features'))\n",
    "        \n",
    "        y_predict_PU_NB = [i.prediction for i in predict_4th_day_NB.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_NB = recall_score(y_label, y_predict_PU_NB)\n",
    "        precision_score_PU_NB = precision_score(y_label, y_predict_PU_NB)\n",
    "        f1_score_PU_NB = f1_score(y_label, y_predict_PU_NB)\n",
    "        tp_NB = 0\n",
    "        tn_NB = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label[i] == 1.0 and y_predict_PU_NB[i] == 1.0:\n",
    "                tp_NB += 1\n",
    "            if y_label[i] == 0.0 and y_predict_PU_NB[i] == 0.0:\n",
    "                tn_NB += 1\n",
    "        accuracy_NB = (tp_NB + tn_NB)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_PU_NB, precision_score_PU_NB, f1_score_PU_NB, accuracy_NB)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_NB: \" + str(recall_score_PU_NB) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_NB: \" + str(precision_score_PU_NB) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_NB: \" + str(f1_score_PU_NB) + '\\n')\n",
    "        log_file2.write(\"accuracy_NB: \" + str(accuracy_NB) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # DT\n",
    "        log_file2.write('###### DecisionTreeClassifier #######\\n')\n",
    "        RN_DT = sc.parallelize(sc.textFile(base_path + \"RN/RN_DT_mean-\" + str(alfa)).map(lambda x: eval(x)).takeSample(False, 240000))\n",
    "        \n",
    "        PU_train_data_DT = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_DT.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "        \n",
    "        DT = DecisionTreeClassifier().fit(PU_train_data_DT)\n",
    "        predict_4th_day_DT = DT.transform(test_data_4thday.select('features'))\n",
    "        \n",
    "        y_predict_PU_DT = [i.prediction for i in predict_4th_day_DT.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_DT = recall_score(y_label, y_predict_PU_DT)\n",
    "        precision_score_PU_DT = precision_score(y_label, y_predict_PU_DT)\n",
    "        f1_score_PU_DT = f1_score(y_label, y_predict_PU_DT)\n",
    "        tp_DT = 0\n",
    "        tn_DT = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label[i] == 1.0 and y_predict_PU_DT[i] == 1.0:\n",
    "                tp_DT += 1\n",
    "            if y_label[i] == 0.0 and y_predict_PU_DT[i] == 0.0:\n",
    "                tn_DT += 1\n",
    "        accuracy_DT = (tp_DT + tn_DT)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_PU_DT, precision_score_PU_DT, f1_score_PU_DT, accuracy_DT)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_DT: \" + str(recall_score_PU_DT) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_DT: \" + str(precision_score_PU_DT) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_DT: \" + str(f1_score_PU_DT) + '\\n')\n",
    "        log_file2.write(\"accuracy_DT: \" + str(accuracy_DT) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # RF\n",
    "        log_file2.write('###### RandomForestClassifier #######\\n')\n",
    "        RN_RF = sc.parallelize(sc.textFile(base_path + \"RN/RN_RF_mean-\" + str(alfa)).map(lambda x: eval(x)).takeSample(False, 240000))\n",
    "        \n",
    "        PU_train_data_RF = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_RF.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "        \n",
    "        RF = RandomForestClassifier().fit(PU_train_data_RF)\n",
    "        predict_4th_day_RF = RF.transform(test_data_4thday.select('features'))\n",
    "        \n",
    "        y_predict_PU_RF = [i.prediction for i in predict_4th_day_RF.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_RF = recall_score(y_label, y_predict_PU_RF)\n",
    "        precision_score_PU_RF = precision_score(y_label, y_predict_PU_RF)\n",
    "        f1_score_PU_RF = f1_score(y_label, y_predict_PU_RF)\n",
    "        tp_RF = 0\n",
    "        tn_RF = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label[i] == 1.0 and y_predict_PU_RF[i] == 1.0:\n",
    "                tp_RF += 1\n",
    "            if y_label[i] == 0.0 and y_predict_PU_RF[i] == 0.0:\n",
    "                tn_RF += 1\n",
    "        accuracy_RF = (tp_RF + tn_RF)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_PU_RF, precision_score_PU_RF, f1_score_PU_RF, accuracy_RF)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_RF: \" + str(recall_score_PU_RF) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_RF: \" + str(precision_score_PU_RF) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_RF: \" + str(f1_score_PU_RF) + '\\n')\n",
    "        log_file2.write(\"accuracy_RF: \" + str(accuracy_RF) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# In Jupyter you have to stop the current context first\\nsc.stop()\\n\\n# Create new config\\nconf = (SparkConf()\\n    .set(\"spark.driver.maxResultSize\", \"8g\"))\\n\\n# Create new context\\nsc = SparkContext(conf=conf)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.get(\"spark.driver.maxResultSize\")\n",
    "\"\"\"# In Jupyter you have to stop the current context first\n",
    "sc.stop()\n",
    "\n",
    "# Create new config\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.driver.maxResultSize\", \"8g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18g'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_file1 = open('qq_PU_alfa_1_v3.log', 'a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file1.write(\"========================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 15:12:46 2017\n",
      "Fri Feb 17 15:13:47 2017\n",
      "Fri Feb 17 15:14:34 2017\n"
     ]
    }
   ],
   "source": [
    "alfa = 0.05\n",
    "train_negative_pairs = sc.textFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa)).map(lambda x: eval(x)).collect()\n",
    "\n",
    "log_file1.write(\"length of train_negative_pairs: \" + str(len(train_negative_pairs)) + '\\n')\n",
    "print(time.asctime())\n",
    "log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "# ===============================================\n",
    "# with PU\n",
    "blta = 0.8  # 从train pos 里面取一部分加到 neg 里面组成PU数据\n",
    "pnsplit = int(train_positive_pairs.count()*blta)   # pnsplit以前的部分作为pu_pos, 以后的部分作为pu_neg\n",
    "pu_pos_features = train_positive_pairs.collect()[:pnsplit]\n",
    "pu_neg_features = train_negative_pairs + train_positive_pairs.collect()[pnsplit:]\n",
    "\n",
    "log_file1.write(\"length of pu_pos_features: \" + str(len(pu_pos_features)) + '\\n')\n",
    "log_file1.write(\"length of pu_neg_features: \" + str(len(pu_neg_features)) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n\\n')\n",
    "print(time.asctime())\n",
    "\n",
    "gamma=0.3  # 从PU数据里面取一部分训练，另一部分作为测试\n",
    "pu_train_pos = sc.parallelize(pu_pos_features).takeSample(False, int(len(pu_pos_features) * gamma))  # list\n",
    "pu_test_pos = sc.parallelize(pu_pos_features).subtractByKey(sc.parallelize(pu_train_pos)).collect()\n",
    "#log_file1.write(\"length of pu_train_pos: \" + str(len(pu_train_pos)) + '\\n')\n",
    "#log_file1.write(\"length of pu_test_pos: \" + str(len(pu_test_pos)) + '\\n')\n",
    "pu_train_neg = sc.parallelize(pu_neg_features, 50).takeSample(False, int(len(pu_neg_features) * gamma))  # list\n",
    "pu_test_neg = sc.parallelize(pu_neg_features).subtractByKey(sc.parallelize(pu_train_neg)).collect()\n",
    "log_file1.write(\"length of pu_train_neg: \" + str(len(pu_train_neg)) + '\\n')\n",
    "log_file1.write(\"length of pu_test_neg: \" + str(len(pu_test_neg)) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n\\n')\n",
    "print(time.asctime())\n",
    "\n",
    "pu_train_data = sc.parallelize(pu_train_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(sc.parallelize(pu_train_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pu_test_data = sc.parallelize(pu_test_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1]), pair=x[0]))\\\n",
    ".union(sc.parallelize(pu_test_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1]), pair=x[0]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### LR\n",
    "log_file1.write(\"###### LogisticRegression ######\\n\")\n",
    "publr = LogisticRegression().fit(pu_train_data)\n",
    "pu_test_data_preRes_LR = publr.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "predict_to_0_LR = pu_test_data_preRes_LR.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "predict_to_0_prob_LR = predict_to_0_LR.map(lambda x: x.probability[0]).collect()\n",
    "\n",
    "prob_LR_max = max(predict_to_0_prob_LR)\n",
    "prob_LR_mean = sum(predict_to_0_prob_LR)/len(predict_to_0_prob_LR)\n",
    "print(len(predict_to_0_prob_LR), prob_LR_max, min(predict_to_0_prob_LR), prob_LR_mean)\n",
    "log_file1.write(\"len(predict_to_0_prob_LR) ++ max(predict_to_0_prob_LR) ++ min(predict_to_0_prob_LR) ++ mean(predict_to_0_prob_LR) \\n\")\n",
    "log_file1.write(str(len(predict_to_0_prob_LR)) + ', ' + str(prob_LR_max) + ', ' + str(min(predict_to_0_prob_LR)) + ', ' + str(prob_LR_mean) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "RN_LR_max = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] >= prob_LR_max).map(lambda x: x.features).collect()\n",
    "RN_LR_mean = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] > prob_LR_mean).map(lambda x: x.features).collect()\n",
    "log_file1.write('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n')\n",
    "log_file1.write(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "print('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n')\n",
    "print(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "sc.parallelize(RN_LR_max).saveAsTextFile(base_path + \"RN/RN_LR_max-\" + str(alfa))\n",
    "sc.parallelize(RN_LR_mean).saveAsTextFile(base_path + \"RN/RN_LR_mean-\" + str(alfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56404 1.0 0.582044445478 0.999992552523\n",
      "len(RN_NB_max): 153265\n",
      "\n",
      "len(RN_NB_mean):155050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### NB\n",
    "log_file1.write(\"###### NaiveBayes ######\\n\")\n",
    "pubNB = NaiveBayes().fit(pu_train_data)\n",
    "pu_test_data_preRes_NB = pubNB.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "predict_to_0_NB = pu_test_data_preRes_NB.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "predict_to_0_prob_NB = predict_to_0_NB.map(lambda x: x.probability[0]).collect()\n",
    "\n",
    "prob_NB_max = max(predict_to_0_prob_NB)\n",
    "prob_NB_mean = sum(predict_to_0_prob_NB)/len(predict_to_0_prob_NB)\n",
    "print(len(predict_to_0_prob_NB), prob_NB_max, min(predict_to_0_prob_NB), prob_NB_mean)\n",
    "log_file1.write(\"len(predict_to_0_prob_NB) ++ max(predict_to_0_prob_NB) ++ min(predict_to_0_prob_NB) ++ mean(predict_to_0_prob_NB) \\n\")\n",
    "log_file1.write(str(len(predict_to_0_prob_NB)) + ', ' + str(prob_NB_max) + ', ' + str(min(predict_to_0_prob_NB)) + ', ' + str(prob_NB_mean) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "RN_NB_max = pu_test_data_preRes_NB.rdd.filter(lambda x: x.probability[0] >= prob_NB_max).map(lambda x: x.features).collect()\n",
    "RN_NB_mean = pu_test_data_preRes_NB.rdd.filter(lambda x: x.probability[0] > prob_NB_mean).map(lambda x: x.features).collect()\n",
    "log_file1.write('len(RN_NB_max): '+str(len(RN_NB_max))+'\\n')\n",
    "log_file1.write(\"len(RN_NB_mean):\" + str(len(RN_NB_mean))+'\\n')\n",
    "print('len(RN_NB_max): '+str(len(RN_NB_max))+'\\n')\n",
    "print(\"len(RN_NB_mean):\" + str(len(RN_NB_mean))+'\\n')\n",
    "sc.parallelize(RN_NB_max).saveAsTextFile(base_path + \"RN/RN_NB_max-\" + str(alfa))\n",
    "sc.parallelize(RN_NB_mean).saveAsTextFile(base_path + \"RN/RN_NB_mean-\" + str(alfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18193 0.999929397229 0.506172839506 0.625787355222\n",
      "len(RN_DT_max): 265276\n",
      "\n",
      "len(RN_DT_mean):289695\n",
      "\n",
      "21187 0.990834485173 0.500017124999 0.614413289861\n",
      "len(RN_RF_max): 94118\n",
      "\n",
      "len(RN_RF_mean):291137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DT\n",
    "log_file1.write(\"###### DecisionTreeClassifier ######\\n\")\n",
    "pubDT = DecisionTreeClassifier().fit(pu_train_data)\n",
    "pu_test_data_preRes_DT = pubDT.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\") \n",
    "\n",
    "predict_to_0_DT = pu_test_data_preRes_DT.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "predict_to_0_prob_DT = predict_to_0_DT.map(lambda x: x.probability[0]).collect()\n",
    "\n",
    "prob_DT_max = max(predict_to_0_prob_DT)\n",
    "prob_DT_mean = sum(predict_to_0_prob_DT)/len(predict_to_0_prob_DT)\n",
    "print(len(predict_to_0_prob_DT), prob_DT_max, min(predict_to_0_prob_DT), prob_DT_mean)\n",
    "log_file1.write(\"len(predict_to_0_prob_DT) ++ max(predict_to_0_prob_DT) ++ min(predict_to_0_prob_DT) ++ mean(predict_to_0_prob_DT) \\n\")\n",
    "log_file1.write(str(len(predict_to_0_prob_DT)) + ', ' + str(prob_DT_max) + ', ' + str(min(predict_to_0_prob_DT)) + ', ' + str(prob_DT_mean) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "RN_DT_max = pu_test_data_preRes_DT.rdd.filter(lambda x: x.probability[0] >= prob_DT_max).map(lambda x: x.features).collect()\n",
    "RN_DT_mean = pu_test_data_preRes_DT.rdd.filter(lambda x: x.probability[0] > prob_DT_mean).map(lambda x: x.features).collect()\n",
    "log_file1.write('len(RN_DT_max): '+str(len(RN_DT_max))+'\\n')\n",
    "log_file1.write(\"len(RN_DT_mean):\" + str(len(RN_DT_mean))+'\\n')\n",
    "print('len(RN_DT_max): '+str(len(RN_DT_max))+'\\n')\n",
    "print(\"len(RN_DT_mean):\" + str(len(RN_DT_mean))+'\\n')\n",
    "sc.parallelize(RN_DT_max).saveAsTextFile(base_path + \"RN/RN_DT_max-\" + str(alfa))\n",
    "sc.parallelize(RN_DT_mean).saveAsTextFile(base_path + \"RN/RN_DT_mean-\" + str(alfa))\n",
    "\n",
    "### RandomForestClassifier\n",
    "log_file1.write(\"###### RandomForestClassifier ######\\n\")\n",
    "pubRF = RandomForestClassifier().fit(pu_train_data)\n",
    "pu_test_data_preRes_RF = pubRF.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features', \"left_outer\")\n",
    "\n",
    "predict_to_0_RF = pu_test_data_preRes_RF.rdd.filter(lambda x: (x.pair.split('-')[0] != 'day123' and x.prediction == 0.0))\n",
    "predict_to_0_prob_RF = predict_to_0_RF.map(lambda x: x.probability[0]).collect()\n",
    "\n",
    "prob_RF_max = max(predict_to_0_prob_RF)\n",
    "prob_RF_mean = sum(predict_to_0_prob_RF)/len(predict_to_0_prob_RF)\n",
    "print(len(predict_to_0_prob_RF), prob_RF_max, min(predict_to_0_prob_RF), prob_RF_mean)\n",
    "log_file1.write(\"len(predict_to_0_prob_RF) ++ max(predict_to_0_prob_RF) ++ min(predict_to_0_prob_RF) ++ mean(predict_to_0_prob_RF) \\n\")\n",
    "log_file1.write(str(len(predict_to_0_prob_RF)) + ', ' + str(prob_RF_max) + ', ' + str(min(predict_to_0_prob_RF)) + ', ' + str(prob_RF_mean) + '\\n')\n",
    "log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "RN_RF_max = pu_test_data_preRes_RF.rdd.filter(lambda x: x.probability[0] >= prob_RF_max).map(lambda x: x.features).collect()\n",
    "RN_RF_mean = pu_test_data_preRes_RF.rdd.filter(lambda x: x.probability[0] > prob_RF_mean).map(lambda x: x.features).collect()\n",
    "log_file1.write('len(RN_RF_max): '+str(len(RN_RF_max))+'\\n')\n",
    "log_file1.write(\"len(RN_RF_mean):\" + str(len(RN_RF_mean))+'\\n')\n",
    "print('len(RN_RF_max): '+str(len(RN_RF_max))+'\\n')\n",
    "print(\"len(RN_RF_mean):\" + str(len(RN_RF_mean))+'\\n')\n",
    "sc.parallelize(RN_RF_max).saveAsTextFile(base_path + \"RN/RN_RF_max-\" + str(alfa))\n",
    "sc.parallelize(RN_RF_mean).saveAsTextFile(base_path + \"RN/RN_RF_mean-\" + str(alfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 15:26:20 2017\n"
     ]
    }
   ],
   "source": [
    "log_file2 = open('qq_PU_alfa_2_v3.log', 'a+')\n",
    "log_file2.write('\\n' + time.asctime() + '\\n')\n",
    "log_file2.write(\"===Predict with RN===\\n\" + '\\n')\n",
    "log_file2.write( \"==========alfa: \" + str(alfa) + \"============\\n\")\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768725361367 0.00723959128279 0.01434409481 0.03524\n",
      "Fri Feb 17 15:30:52 2017\n",
      "0.993648707841 0.010038876584 0.0198769356135 0.10513\n",
      "Fri Feb 17 15:37:15 2017\n",
      "0.916119141481 0.0638869797633 0.119444325467 0.87665\n",
      "Fri Feb 17 15:43:23 2017\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "log_file2.write('###### NaiveBayes #######\\n')\n",
    "RN_NB = sc.textFile(base_path + \"RN/RN_NB_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "\n",
    "PU_train_data_NB = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(RN_NB.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "\n",
    "NB = NaiveBayes().fit(PU_train_data_NB)\n",
    "predict_4th_day_NB = NB.transform(test_data_4thday.select('features'))\n",
    "\n",
    "y_predict_PU_NB = [i.prediction for i in predict_4th_day_NB.select('prediction').collect()]\n",
    "\n",
    "recall_score_PU_NB = recall_score(y_label, y_predict_PU_NB)\n",
    "precision_score_PU_NB = precision_score(y_label, y_predict_PU_NB)\n",
    "f1_score_PU_NB = f1_score(y_label, y_predict_PU_NB)\n",
    "tp_NB = 0\n",
    "tn_NB = 0\n",
    "for i in range(500000):\n",
    "    if y_label[i] == 1.0 and y_predict_PU_NB[i] == 1.0:\n",
    "        tp_NB += 1\n",
    "    if y_label[i] == 0.0 and y_predict_PU_NB[i] == 0.0:\n",
    "        tn_NB += 1\n",
    "accuracy_NB = (tp_NB + tn_NB)/500000\n",
    "\n",
    "print(recall_score_PU_NB, precision_score_PU_NB, f1_score_PU_NB, accuracy_NB)\n",
    "\n",
    "log_file2.write('\\n')\n",
    "log_file2.write(\"recall_score_PU_NB: \" + str(recall_score_PU_NB) + '\\n')\n",
    "log_file2.write(\"precision_score_PU_NB: \" + str(precision_score_PU_NB) + '\\n')\n",
    "log_file2.write(\"f1_score_PU_NB: \" + str(f1_score_PU_NB) + '\\n')\n",
    "log_file2.write(\"accuracy_NB: \" + str(accuracy_NB) + '\\n')\n",
    "log_file2.write(time.asctime() + '\\n')\n",
    "log_file2.write('\\n')\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "# DT\n",
    "log_file2.write('###### DecisionTreeClassifier #######\\n')\n",
    "RN_DT = sc.textFile(base_path + \"RN/RN_DT_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "\n",
    "PU_train_data_DT = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(RN_DT.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "\n",
    "DT = DecisionTreeClassifier().fit(PU_train_data_DT)\n",
    "predict_4th_day_DT = DT.transform(test_data_4thday.select('features'))\n",
    "\n",
    "y_predict_PU_DT = [i.prediction for i in predict_4th_day_DT.select('prediction').collect()]\n",
    "\n",
    "recall_score_PU_DT = recall_score(y_label, y_predict_PU_DT)\n",
    "precision_score_PU_DT = precision_score(y_label, y_predict_PU_DT)\n",
    "f1_score_PU_DT = f1_score(y_label, y_predict_PU_DT)\n",
    "tp_DT = 0\n",
    "tn_DT = 0\n",
    "for i in range(500000):\n",
    "    if y_label[i] == 1.0 and y_predict_PU_DT[i] == 1.0:\n",
    "        tp_DT += 1\n",
    "    if y_label[i] == 0.0 and y_predict_PU_DT[i] == 0.0:\n",
    "        tn_DT += 1\n",
    "accuracy_DT = (tp_DT + tn_DT)/500000\n",
    "\n",
    "print(recall_score_PU_DT, precision_score_PU_DT, f1_score_PU_DT, accuracy_DT)\n",
    "\n",
    "log_file2.write('\\n')\n",
    "log_file2.write(\"recall_score_PU_DT: \" + str(recall_score_PU_DT) + '\\n')\n",
    "log_file2.write(\"precision_score_PU_DT: \" + str(precision_score_PU_DT) + '\\n')\n",
    "log_file2.write(\"f1_score_PU_DT: \" + str(f1_score_PU_DT) + '\\n')\n",
    "log_file2.write(\"accuracy_DT: \" + str(accuracy_DT) + '\\n')\n",
    "log_file2.write(time.asctime() + '\\n')\n",
    "log_file2.write('\\n')\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "# RF\n",
    "log_file2.write('###### RandomForestClassifier #######\\n')\n",
    "RN_RF = sc.textFile(base_path + \"RN/RN_RF_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "\n",
    "PU_train_data_RF = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(RN_RF.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "\n",
    "RF = RandomForestClassifier().fit(PU_train_data_RF)\n",
    "predict_4th_day_RF = RF.transform(test_data_4thday.select('features'))\n",
    "\n",
    "y_predict_PU_RF = [i.prediction for i in predict_4th_day_RF.select('prediction').collect()]\n",
    "\n",
    "recall_score_PU_RF = recall_score(y_label, y_predict_PU_RF)\n",
    "precision_score_PU_RF = precision_score(y_label, y_predict_PU_RF)\n",
    "f1_score_PU_RF = f1_score(y_label, y_predict_PU_RF)\n",
    "tp_RF = 0\n",
    "tn_RF = 0\n",
    "for i in range(500000):\n",
    "    if y_label[i] == 1.0 and y_predict_PU_RF[i] == 1.0:\n",
    "        tp_RF += 1\n",
    "    if y_label[i] == 0.0 and y_predict_PU_RF[i] == 0.0:\n",
    "        tn_RF += 1\n",
    "accuracy_RF = (tp_RF + tn_RF)/500000\n",
    "\n",
    "print(recall_score_PU_RF, precision_score_PU_RF, f1_score_PU_RF, accuracy_RF)\n",
    "\n",
    "log_file2.write('\\n')\n",
    "log_file2.write(\"recall_score_PU_RF: \" + str(recall_score_PU_RF) + '\\n')\n",
    "log_file2.write(\"precision_score_PU_RF: \" + str(precision_score_PU_RF) + '\\n')\n",
    "log_file2.write(\"f1_score_PU_RF: \" + str(f1_score_PU_RF) + '\\n')\n",
    "log_file2.write(\"accuracy_RF: \" + str(accuracy_RF) + '\\n')\n",
    "log_file2.write(time.asctime() + '\\n')\n",
    "log_file2.write('\\n')\n",
    "\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 11:22:56 2017\n",
      "Fri Feb 17 11:24:06 2017\n",
      "Fri Feb 17 11:24:52 2017\n"
     ]
    }
   ],
   "source": [
    "alfa = 0.05\n",
    "train_negative_pairs = sc.textFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa)).map(lambda x: eval(x)).collect()\n",
    "\n",
    "#log_file1.write(\"length of train_negative_pairs: \" + str(len(train_negative_pairs)) + '\\n')\n",
    "print(time.asctime())\n",
    "#log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "# ===============================================\n",
    "# with PU\n",
    "blta = 0.8  # 从train pos 里面取一部分加到 neg 里面组成PU数据\n",
    "pnsplit = int(train_positive_pairs.count()*blta)   # pnsplit以前的部分作为pu_pos, 以后的部分作为pu_neg\n",
    "pu_pos_features = train_positive_pairs.collect()[:pnsplit]\n",
    "pu_neg_features = train_negative_pairs + train_positive_pairs.collect()[pnsplit:]\n",
    "\n",
    "#log_file1.write(\"length of pu_pos_features: \" + str(len(pu_pos_features)) + '\\n')\n",
    "#log_file1.write(\"length of pu_neg_features: \" + str(len(pu_neg_features)) + '\\n')\n",
    "#log_file1.write(time.asctime() + '\\n\\n')\n",
    "print(time.asctime())\n",
    "\n",
    "gamma=0.3  # 从PU数据里面取一部分训练，另一部分作为测试\n",
    "pu_train_pos = sc.parallelize(pu_pos_features).takeSample(False, int(len(pu_pos_features) * gamma))  # list\n",
    "pu_test_pos = sc.parallelize(pu_pos_features).subtractByKey(sc.parallelize(pu_train_pos)).collect()\n",
    "#log_file1.write(\"length of pu_train_pos: \" + str(len(pu_train_pos)) + '\\n')\n",
    "#log_file1.write(\"length of pu_test_pos: \" + str(len(pu_test_pos)) + '\\n')\n",
    "pu_train_neg = sc.parallelize(pu_neg_features, 50).takeSample(False, int(len(pu_neg_features) * gamma))  # list\n",
    "pu_test_neg = sc.parallelize(pu_neg_features).subtractByKey(sc.parallelize(pu_train_neg)).collect()\n",
    "#log_file1.write(\"length of pu_train_neg: \" + str(len(pu_train_neg)) + '\\n')\n",
    "#log_file1.write(\"length of pu_test_neg: \" + str(len(pu_test_neg)) + '\\n')\n",
    "#log_file1.write(time.asctime() + '\\n\\n')\n",
    "print(time.asctime())\n",
    "\n",
    "pu_train_data = sc.parallelize(pu_train_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(sc.parallelize(pu_train_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\n",
    "\n",
    "pu_test_data = sc.parallelize(pu_test_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(sc.parallelize(pu_test_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pu_test_data = sc.parallelize(pu_test_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1]), pair=x[0]))\\\n",
    ".union(sc.parallelize(pu_test_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1]), pair=x[0]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454320"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu_test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|                pair|\n",
      "+--------------------+-----+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|day1-861381339898...|\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|day3-861385194265...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pu_test_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 11:25:04 2017\n",
      "Fri Feb 17 11:25:56 2017\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())\n",
    "pu_spy = [Vectors.dense(x[1]) for x in train_positive_pairs.collect()[pnsplit:]]\n",
    "publr = LogisticRegression().fit(pu_train_data)\n",
    "pu_test_data_preRes_LR = publr.transform(pu_test_data.select(\"features\")) \n",
    "\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47118"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pu_spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,0.0,0.0,...|[0.80226282901117...|[0.69045831399825...|       0.0|\n",
      "|[0.0,0.0,0.0,0.0,...|[-40.936007754644...|[1.66616390165119...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pu_test_data_preRes_LR.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454320"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu_test_data_preRes_LR.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaaaa = pu_test_data_preRes_LR.join(pu_test_data, 'features', \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454460"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaaaa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+\n",
      "|            features|       rawPrediction|         probability|prediction|label|                pair|\n",
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|[0.44888708795668...|[0.61037459658138...|       0.0|  0.0|day2-861395169813...|\n",
      "|[0.0,0.0,0.0,0.0,...|[3.06571481451773...|[0.95545615175954...|       0.0|  0.0|day123-8613951946...|\n",
      "|[0.0,0.0,0.0,0.0,...|[2.39139678017373...|[0.91616890800357...|       0.0|  0.0|day123-8613951680...|\n",
      "|[0.0,0.0,0.0,0.0,...|[2.60683271638322...|[0.93130002918038...|       0.0|  0.0|day123-8613951708...|\n",
      "|[0.0,0.0,0.0,0.0,...|[0.60560027801470...|[0.64693651394611...|       0.0|  0.0|day2-861392143050...|\n",
      "+--------------------+--------------------+--------------------+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aaaaa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 4.0, 0.0, 0.0, 5.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 1.0, 0.0, 0.0, 0.0, 0.0, 5.0, 25.0, 85.0, 100.0, 79109.0, 1037.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 156.0, 147.0, 180.0, 142.0, 99.0, 117.0, 117.0, 203.0, 333.0, 349.0, 413.0, 453.0, 455.0, 463.0, 502.0, 407.0, 506.0, 405.0, 432.0, 472.0, 460.0, 375.0, 353.0, 260.0, 7799.0, 43548.0, 31033.0, 54799406.0]), rawPrediction=DenseVector([0.9557, -0.9557]), probability=DenseVector([0.7223, 0.2777]), prediction=0.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 2.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 10.0, 41.0, 38.0, 10058.0, 1037.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 156.0, 147.0, 180.0, 142.0, 99.0, 117.0, 117.0, 203.0, 333.0, 349.0, 413.0, 453.0, 455.0, 463.0, 502.0, 407.0, 506.0, 405.0, 432.0, 472.0, 460.0, 375.0, 353.0, 260.0, 7799.0, 43548.0, 31033.0, 54799406.0]), rawPrediction=DenseVector([0.7383, -0.7383]), probability=DenseVector([0.6766, 0.3234]), prediction=0.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 9.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 17.0, 62.0, 83.0, 50815.0, 981.0, 223.0, 1057.0, 0.0, 0.0, 0.0, 830.0, 9990.0, 1.0, 75015.0, 2.0, 1740.0, 970.0, 2744.0, 17.0, 1.0, 0.0, 0.0, 0.0, 57.0, 73428.0, 2554.0, 0.0, 0.0, 17.0, 0.0, 2.0, 1.0, 7278.0, 0.0, 0.0, 0.0, 797.0, 114.0, 94.0, 2616.0, 1739.0, 830.0, 1.0, 223.0, 2.0, 0.0, 1.0, 0.0, 190.0, 1.0, 4.0, 1056.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1581.0, 0.0, 0.0, 3013.0, 1812.0, 1459.0, 1266.0, 1545.0, 1776.0, 2086.0, 3449.0, 4386.0, 4312.0, 4484.0, 4419.0, 4289.0, 4271.0, 4625.0, 4049.0, 3969.0, 4789.0, 4736.0, 4705.0, 5882.0, 5599.0, 6080.0, 5588.0, 92589.0, 674954.0, 622912.0, 440580176.0]), rawPrediction=DenseVector([-20.8446, 20.8446]), probability=DenseVector([0.0, 1.0]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.0, 0.0, 0.0, 0.0, 26.0, 0.0, 0.0, 0.0, 0.0, 26.0, 72.0, 1283.0, 902.0, 409932.0, 743.0, 0.0, 33.0, 0.0, 0.0, 0.0, 2.0, 123.0, 0.0, 5980.0, 0.0, 0.0, 2.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 185.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 5980.0, 33.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 306.0, 150.0, 58.0, 81.0, 70.0, 53.0, 168.0, 307.0, 284.0, 245.0, 267.0, 168.0, 335.0, 207.0, 278.0, 241.0, 268.0, 278.0, 344.0, 357.0, 397.0, 396.0, 465.0, 612.0, 6335.0, 57860.0, 53226.0, 46377859.0]), rawPrediction=DenseVector([-4.0503, 4.0503]), probability=DenseVector([0.0171, 0.9829]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 13.0, 11.0, 24.0, 30.0, 17.0, 21.0, 17.0, 18.0, 18.0, 0.0, 28.0, 30.0, 3.0, 6.0, 38.0, 16.0, 36.0, 17.0, 27.0, 15.0, 0.0, 0.0, 0.0, 41.0, 411.0, 1866.0, 2148.0, 1599665.0, 146.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.0, 0.0, 550.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 550.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 1.0, 4.0, 1.0, 8.0, 2.0, 4.0, 8.0, 38.0, 25.0, 20.0, 45.0, 46.0, 21.0, 54.0, 39.0, 58.0, 32.0, 35.0, 42.0, 31.0, 19.0, 44.0, 16.0, 607.0, 2974.0, 2297.0, 2094077.0]), rawPrediction=DenseVector([-0.0079, 0.0079]), probability=DenseVector([0.498, 0.502]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 7.0, 0.0, 0.0, 0.0, 27.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 68.0, 354.0, 417.0, 308099.0, 296.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 608.0, 0.0, 686.0, 0.0, 0.0, 0.0, 28.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 267.0, 0.0, 0.0, 0.0, 0.0, 0.0, 341.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 0.0, 686.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.0, 33.0, 4.0, 26.0, 1.0, 7.0, 4.0, 25.0, 23.0, 146.0, 78.0, 109.0, 91.0, 70.0, 38.0, 25.0, 84.0, 67.0, 59.0, 133.0, 72.0, 90.0, 66.0, 56.0, 1322.0, 12174.0, 13604.0, 8626085.0]), rawPrediction=DenseVector([0.1372, -0.1372]), probability=DenseVector([0.5343, 0.4657]), prediction=0.0), Row(features=DenseVector([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 3.0, 3.0, 5.0, 3.0, 5.0, 2.0, 3.0, 0.0, 3.0, 1.0, 7.0, 2.0, 3.0, 7.0, 7.0, 0.0, 9.0, 58.0, 268.0, 292.0, 144132.0, 1786.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3775.0, 0.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3775.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 107.0, 42.0, 46.0, 42.0, 33.0, 35.0, 74.0, 113.0, 167.0, 200.0, 199.0, 226.0, 204.0, 202.0, 201.0, 190.0, 225.0, 229.0, 256.0, 248.0, 212.0, 215.0, 186.0, 143.0, 3795.0, 16797.0, 13995.0, 9645284.0]), rawPrediction=DenseVector([-2.4712, 2.4712]), probability=DenseVector([0.0779, 0.9221]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.0, 6.0, 0.0, 10.0, 6.0, 6.0, 2.0, 0.0, 1.0, 19.0, 46.0, 237.0, 272.0, 349899.0, 778.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 906.0, 0.0, 2783.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 906.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2783.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 139.0, 77.0, 65.0, 30.0, 49.0, 52.0, 93.0, 140.0, 191.0, 306.0, 157.0, 175.0, 159.0, 165.0, 194.0, 217.0, 168.0, 195.0, 224.0, 203.0, 183.0, 147.0, 188.0, 173.0, 3690.0, 21130.0, 15821.0, 15253634.0]), rawPrediction=DenseVector([-2.2405, 2.2405]), probability=DenseVector([0.0962, 0.9038]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0, 54.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 72.0, 131.0, 11.0, 79.0, 53.0, 0.0, 0.0, 63.0, 425.0, 3191.0, 3668.0, 3388877.0, 314.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 124.0, 0.0, 0.0, 693.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 693.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 124.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 18.0, 13.0, 14.0, 2.0, 4.0, 6.0, 27.0, 41.0, 48.0, 45.0, 42.0, 65.0, 34.0, 36.0, 46.0, 58.0, 37.0, 34.0, 43.0, 60.0, 39.0, 39.0, 47.0, 838.0, 4658.0, 3970.0, 3367283.0]), rawPrediction=DenseVector([-1.0893, 1.0893]), probability=DenseVector([0.2517, 0.7483]), prediction=1.0), Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 19.0, 9.0, 12.0, 18.0, 2.0, 2.0, 5.0, 8.0, 6.0, 6.0, 12.0, 0.0, 3.0, 3.0, 0.0, 24.0, 114.0, 801.0, 803.0, 601679.0, 1786.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3775.0, 0.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3775.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 107.0, 42.0, 46.0, 42.0, 33.0, 35.0, 74.0, 113.0, 167.0, 200.0, 199.0, 226.0, 204.0, 202.0, 201.0, 190.0, 225.0, 229.0, 256.0, 248.0, 212.0, 215.0, 186.0, 143.0, 3795.0, 16797.0, 13995.0, 9645284.0]), rawPrediction=DenseVector([-3.3951, 3.3951]), probability=DenseVector([0.0324, 0.9676]), prediction=1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(pu_test_data_preRes_LR.rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = [Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 4.0, 0.0, 0.0, 5.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 1.0, 0.0, 0.0, 0.0, 0.0, 5.0, 25.0, 85.0, 100.0, 79109.0, 1037.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 156.0, 147.0, 180.0, 142.0, 99.0, 117.0, 117.0, 203.0, 333.0, 349.0, 413.0, 453.0, 455.0, 463.0, 502.0, 407.0, 506.0, 405.0, 432.0, 472.0, 460.0, 375.0, 353.0, 260.0, 7799.0, 43548.0, 31033.0, 54799406.0]),Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 2.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 10.0, 41.0, 38.0, 10058.0, 1037.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7725.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 156.0, 147.0, 180.0, 142.0, 99.0, 117.0, 117.0, 203.0, 333.0, 349.0, 413.0, 453.0, 455.0, 463.0, 502.0, 407.0, 506.0, 405.0, 432.0, 472.0, 460.0, 375.0, 353.0, 260.0, 7799.0, 43548.0, 31033.0, 54799406.0]), Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.0, 0.0, 0.0, 0.0, 26.0, 0.0, 0.0, 0.0, 0.0, 26.0, 72.0, 1283.0, 902.0, 409932.0, 743.0, 0.0, 33.0, 0.0, 0.0, 0.0, 2.0, 123.0, 0.0, 5980.0, 0.0, 0.0, 2.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 185.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 111.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 5980.0, 33.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 306.0, 150.0, 58.0, 81.0, 70.0, 53.0, 168.0, 307.0, 284.0, 245.0, 267.0, 168.0, 335.0, 207.0, 278.0, 241.0, 268.0, 278.0, 344.0, 357.0, 397.0, 396.0, 465.0, 612.0, 6335.0, 57860.0, 53226.0, 46377859.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_predict_to_0(x):\n",
    "    if x.pair.split('-')[0] != 'day123' and x.prediction == 0.0:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 11:25:56 2017\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())\n",
    "predict_to_0_LR = aaaaa.rdd.filter(lambda x: pos_predict_to_0(x))#(x.features in xxx and x.prediction == 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 5.0, 6.0, 32.0, 34.0, 28751.0, 997.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2510.0, 0.0, 42.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2510.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 97.0, 66.0, 58.0, 21.0, 29.0, 35.0, 39.0, 104.0, 101.0, 131.0, 128.0, 143.0, 100.0, 121.0, 112.0, 123.0, 120.0, 134.0, 128.0, 162.0, 156.0, 145.0, 151.0, 148.0, 2552.0, 12085.0, 8994.0, 114166357.0]), rawPrediction=DenseVector([0.4489, -0.4489]), probability=DenseVector([0.6104, 0.3896]), prediction=0.0, label=0.0, pair='day2-8613951698130-120.198.196.11'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 13.0, 14.0, 4.0, 0.0, 17.0, 1.0, 2.0, 1.0, 0.0, 18.0, 59.0, 434.0, 397.0, 78078.0, 1641.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1121.0, 0.0, 19991.0, 0.0, 0.0, 0.0, 232.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 56.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 506.0, 0.0, 0.0, 0.0, 0.0, 0.0, 615.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 173.0, 0.0, 19991.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 626.0, 483.0, 235.0, 60.0, 29.0, 92.0, 110.0, 367.0, 750.0, 1045.0, 1554.0, 2011.0, 1336.0, 786.0, 626.0, 815.0, 768.0, 784.0, 1534.0, 1628.0, 2129.0, 1304.0, 1209.0, 1065.0, 21346.0, 283649.0, 269742.0, 1271199391.0]), rawPrediction=DenseVector([0.6056, -0.6056]), probability=DenseVector([0.6469, 0.3531]), prediction=0.0, label=0.0, pair='day2-8613921430506-183.207.233.95'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 50.0, 0.0, 0.0, 36.0, 221.0, 137.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0, 455.0, 3341.0, 3281.0, 2676536.0, 405.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 877.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 877.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 6.0, 16.0, 11.0, 12.0, 3.0, 14.0, 53.0, 57.0, 72.0, 33.0, 45.0, 36.0, 48.0, 47.0, 51.0, 33.0, 48.0, 63.0, 37.0, 43.0, 29.0, 53.0, 37.0, 877.0, 5023.0, 3740.0, 2465220.0]), rawPrediction=DenseVector([0.1369, -0.1369]), probability=DenseVector([0.5342, 0.4658]), prediction=0.0, label=1.0, pair='day3-8613951085451-117.135.128.15'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 5.0, 6.0, 0.0, 7.0, 2.0, 0.0, 5.0, 2.0, 3.0, 1.0, 4.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20.0, 56.0, 203.0, 239.0, 161750.0, 366.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 796.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 796.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 19.0, 18.0, 10.0, 8.0, 5.0, 12.0, 19.0, 36.0, 52.0, 34.0, 47.0, 62.0, 31.0, 32.0, 28.0, 35.0, 59.0, 45.0, 37.0, 45.0, 46.0, 40.0, 58.0, 796.0, 4465.0, 3392.0, 102802237.0]), rawPrediction=DenseVector([1.5882, -1.5882]), probability=DenseVector([0.8304, 0.1696]), prediction=0.0, label=1.0, pair='day2-8613951085451-117.135.128.31'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 9.0, 8.0, 4.0, 15.0, 33.0, 114.0, 136.0, 77398.0, 105.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.0, 0.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 115.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 4.0, 2.0, 0.0, 0.0, 3.0, 11.0, 5.0, 5.0, 5.0, 18.0, 14.0, 13.0, 12.0, 6.0, 6.0, 9.0, 4.0, 10.0, 15.0, 5.0, 9.0, 7.0, 165.0, 922.0, 668.0, 1495825.0]), rawPrediction=DenseVector([1.9005, -1.9005]), probability=DenseVector([0.8699, 0.1301]), prediction=0.0, label=0.0, pair='day2-8613961116217-221.179.19.136'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 6.0, 8.0, 33.0, 38.0, 29165.0, 1530.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 188.0, 0.0, 13110.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 188.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13110.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 807.0, 375.0, 429.0, 240.0, 230.0, 178.0, 187.0, 383.0, 552.0, 576.0, 664.0, 657.0, 660.0, 582.0, 544.0, 593.0, 574.0, 594.0, 743.0, 663.0, 753.0, 826.0, 861.0, 627.0, 13298.0, 73841.0, 55655.0, 162088009.0]), rawPrediction=DenseVector([0.5776, -0.5776]), probability=DenseVector([0.6405, 0.3595]), prediction=0.0, label=1.0, pair='day2-8613951661583-120.204.200.174'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 11.0, 8.0, 11.0, 0.0, 0.0, 11.0, 37.0, 0.0, 0.0, 4.0, 1.0, 12.0, 29.0, 1.0, 10.0, 7.0, 9.0, 28.0, 153.0, 826.0, 736.0, 754075.0, 82.0, 0.0, 0.0, 0.0, 0.0, 6.0, 841.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 841.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 240.0, 21.0, 12.0, 11.0, 0.0, 5.0, 19.0, 38.0, 41.0, 25.0, 68.0, 32.0, 23.0, 46.0, 5.0, 27.0, 20.0, 13.0, 32.0, 33.0, 27.0, 21.0, 24.0, 64.0, 847.0, 4845.0, 4032.0, 2957490.0]), rawPrediction=DenseVector([1.1286, -1.1286]), probability=DenseVector([0.7556, 0.2444]), prediction=0.0, label=1.0, pair='day3-8613915957883-120.198.189.123'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 16.0, 31.0, 0.0, 11.0, 1.0, 1.0, 6.0, 5.0, 8.0, 0.0, 0.0, 0.0, 0.0, 25.0, 81.0, 417.0, 520.0, 647068.0, 386.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 882.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 882.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.0, 28.0, 4.0, 9.0, 11.0, 15.0, 17.0, 26.0, 45.0, 47.0, 36.0, 34.0, 54.0, 18.0, 31.0, 57.0, 44.0, 52.0, 48.0, 38.0, 55.0, 37.0, 32.0, 125.0, 882.0, 4897.0, 4333.0, 2129646.0]), rawPrediction=DenseVector([0.6687, -0.6687]), probability=DenseVector([0.6612, 0.3388]), prediction=0.0, label=1.0, pair='day1-8613913304443-117.135.128.17'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 12.0, 0.0, 0.0, 5.0, 14.0, 80.0, 91.0, 43143.0, 331.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1647.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1647.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 17.0, 23.0, 16.0, 12.0, 19.0, 21.0, 21.0, 86.0, 103.0, 96.0, 126.0, 73.0, 90.0, 86.0, 139.0, 120.0, 98.0, 68.0, 105.0, 119.0, 63.0, 55.0, 60.0, 1662.0, 9960.0, 8081.0, 11547614.0]), rawPrediction=DenseVector([2.358, -2.358]), probability=DenseVector([0.9136, 0.0864]), prediction=0.0, label=1.0, pair='day2-8613921784893-101.227.131.102'),\n",
       " Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 10.0, 11.0, 7.0, 0.0, 0.0, 11.0, 30.0, 121.0, 156.0, 122885.0, 150.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 142.0, 0.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 142.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 5.0, 2.0, 5.0, 3.0, 1.0, 3.0, 4.0, 8.0, 6.0, 15.0, 11.0, 19.0, 10.0, 13.0, 10.0, 16.0, 18.0, 17.0, 8.0, 5.0, 12.0, 7.0, 7.0, 213.0, 1281.0, 961.0, 102575592.0]), rawPrediction=DenseVector([2.7313, -2.7313]), probability=DenseVector([0.9388, 0.0612]), prediction=0.0, label=0.0, pair='day2-8613913006776-221.179.19.157')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_to_0_LR.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_to_0_prob_LR = predict_to_0_LR.map(lambda x: x.probability[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict_to_0_prob_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63587 0.999993652141 0.500028466335 0.741877658985\n"
     ]
    }
   ],
   "source": [
    "prob_LR_max = max(predict_to_0_prob_LR)\n",
    "prob_LR_mean = sum(predict_to_0_prob_LR)/len(predict_to_0_prob_LR)\n",
    "print(len(predict_to_0_prob_LR), prob_LR_max, min(predict_to_0_prob_LR), prob_LR_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RN_LR_max = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] >= prob_LR_max).map(lambda x: x.features).collect()\n",
    "RN_LR_mean = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] > prob_LR_mean).map(lambda x: x.features).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 299914\n",
      "Fri Feb 17 11:28:07 2017\n"
     ]
    }
   ],
   "source": [
    "print(len(RN_LR_max), len(RN_LR_mean))\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 11:33:48 2017\n",
      "0.877792378449 0.0635313138999 0.118486985056 0.880726\n"
     ]
    }
   ],
   "source": [
    "PU_train_data_LR = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    ".union(sc.parallelize(RN_LR_mean).map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "\n",
    "lr = LogisticRegression().fit(PU_train_data_LR)\n",
    "predict_4th_day_LR = lr.transform(test_data_4thday.select('features'))\n",
    "\n",
    "y_predict_PU_LR = [i.prediction for i in predict_4th_day_LR.select('prediction').collect()]\n",
    "\n",
    "recall_score_PU_LR = recall_score(y_label, y_predict_PU_LR)\n",
    "precision_score_PU_LR = precision_score(y_label, y_predict_PU_LR)\n",
    "f1_score_PU_LR = f1_score(y_label, y_predict_PU_LR)\n",
    "tp_LR = 0\n",
    "tn_LR = 0\n",
    "for i in range(500000):\n",
    "    if y_label[i] == 1.0 and y_predict_PU_LR[i] == 1.0:\n",
    "        tp_LR += 1\n",
    "    if y_label[i] == 0.0 and y_predict_PU_LR[i] == 0.0:\n",
    "        tn_LR += 1\n",
    "accuracy_LR = (tp_LR + tn_LR)/500000\n",
    "\n",
    "print(time.asctime())\n",
    "print(recall_score_PU_LR, precision_score_PU_LR, f1_score_PU_LR, accuracy_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 16 21:41:55 2017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b31033cdbd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredict_to_0_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpu_test_data_preRes_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpu_spy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_to_0_prob_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_to_0_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \"\"\"predict_to_0_prob_LR = []\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpu_test_data_preRes_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wbb/anaconda3/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "predict_to_0_prob_LR = predict_to_0_LR.map(lambda x: x.probability[0]).collect()\n",
    "\"\"\"predict_to_0_prob_LR = []\n",
    "for i in pu_test_data_preRes_LR.collect():\n",
    "    if i.prediction == 0.0 and i.features in pu_spy:\n",
    "        predict_to_0_prob_LR.append(i.probability[0])\"\"\"\n",
    "prob_LR_max = max(predict_to_0_prob_LR)\n",
    "prob_LR_mean = sum(predict_to_0_prob_LR)/len(predict_to_0_prob_LR)\n",
    "print(len(predict_to_0_prob_LR), prob_LR_max, min(predict_to_0_prob_LR), prob_LR_mean)\n",
    "#RN_LR_max = []\n",
    "#RN_LR_mean = []\n",
    "RN_LR_max = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] >= prob_LR_max).map(lambda x: x.features).collect()\n",
    "RN_LR_mean = pu_test_data_preRes_LR.rdd.filter(lambda x: x.probability[0] > prob_LR_mean).map(lambda x: x.features).collect()\n",
    "\n",
    "\"\"\"for i in pu_test_data_preRes_LR.collect():\n",
    "    if i.probability[0] >= prob_LR_max:\n",
    "        RN_LR_max.append(i.features)\n",
    "    if i.probability[0] > prob_LR_mean:\n",
    "        RN_LR_mean.append(i.features)\"\"\"\n",
    "print(len(RN_LR_max), len(RN_LR_mean))\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(RN_LR_max[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pu_test_data_preRes_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 15 18:02:04 2017\n",
      "Wed Feb 15 18:02:24 2017\n",
      "Wed Feb 15 18:03:22 2017\n",
      "293895 1.0 0.500005465819 0.893220571531\n",
      "118258 1.0 0.506136879818 0.999646331541\n",
      "295498 0.999885225928 0.588679245283 0.980899639987\n"
     ]
    }
   ],
   "source": [
    "with open('qq_PU_alfa_1_v2.log', 'a+') as log_file1:\n",
    "    # \n",
    "    for alfa in [0.05]:# 0.10, 0.15, 0.20, 0.25, 0.30, 0.35]:\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        log_file1.write(\"===GET RN===\\n\" + '\\n')\n",
    "        log_file1.write( \"==========alfa: \" + str(alfa) + \"============\\n\")\n",
    "        print(time.asctime())\n",
    "        # train_unlabeled -> train_neg\n",
    "        #train_negative_pairs = train_unlabled_pairs.takeSample(False, int(alfa * 8268837))\n",
    "        #sc.parallelize(train_negative_pairs).coalesce(20).saveAsTextFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa))\n",
    "        train_negative_pairs = sc.textFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa)).map(lambda x: eval(x)).collect()\n",
    "\n",
    "        log_file1.write(\"length of train_negative_pairs: \" + str(len(train_negative_pairs)) + '\\n')\n",
    "        print(time.asctime())\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "\n",
    "        # ===============================================\n",
    "        # with PU\n",
    "        blta = 0.8  # 从train pos 里面取一部分加到 neg 里面组成PU数据\n",
    "        pnsplit = int(train_positive_pairs.count()*blta)   # pnsplit以前的部分作为pu_pos, 以后的部分作为pu_neg\n",
    "        pu_pos_features = train_positive_pairs.collect()[:pnsplit]\n",
    "        pu_neg_features = train_negative_pairs + train_positive_pairs.collect()[pnsplit:]\n",
    "        \n",
    "        log_file1.write(\"length of pu_pos_features: \" + str(len(pu_pos_features)) + '\\n')\n",
    "        log_file1.write(\"length of pu_neg_features: \" + str(len(pu_neg_features)) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n\\n')\n",
    "        print(time.asctime())\n",
    "\n",
    "        gamma=0.3  # 从PU数据里面取一部分训练，另一部分作为测试\n",
    "        pu_train_pos = sc.parallelize(pu_pos_features).takeSample(False, int(len(pu_pos_features) * gamma))  # list\n",
    "        pu_test_pos = sc.parallelize(pu_pos_features).subtractByKey(sc.parallelize(pu_train_pos)).collect()\n",
    "        log_file1.write(\"length of pu_train_pos: \" + str(len(pu_train_pos)) + '\\n')\n",
    "        log_file1.write(\"length of pu_test_pos: \" + str(len(pu_test_pos)) + '\\n')\n",
    "        pu_train_neg = sc.parallelize(pu_neg_features, 50).takeSample(False, int(len(pu_neg_features) * gamma))  # list\n",
    "        pu_test_neg = sc.parallelize(pu_neg_features).subtractByKey(sc.parallelize(pu_train_neg)).collect()\n",
    "        log_file1.write(\"length of pu_train_neg: \" + str(len(pu_train_neg)) + '\\n')\n",
    "        log_file1.write(\"length of pu_test_neg: \" + str(len(pu_test_neg)) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n\\n')\n",
    "        print(time.asctime())\n",
    "\n",
    "        pu_train_data = sc.parallelize(pu_train_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(sc.parallelize(pu_train_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\n",
    "\n",
    "        pu_test_data = sc.parallelize(pu_test_pos).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(sc.parallelize(pu_test_neg).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\n",
    "        \n",
    "        pu_test_data_list = pu_test_data.collect()\n",
    "        length_pu_test_data = len(pu_test_data)\n",
    "\n",
    "        ### LR\n",
    "        publr = LogisticRegression().fit(pu_train_data)\n",
    "        pu_test_data_preRes_LR = publr.transform(pu_test_data.select(\"features\")) #.join(pu_test_data, 'features')\n",
    "\n",
    "        pu_test_data_preRes_LR_temp = pu_test_data_preRes_LR.filter((col('prediction') == 0) & (col('label') == 0))\\\n",
    "        .select('features', 'probability').collect()\n",
    "\n",
    "        predict_to_0_prob_LR = []\n",
    "        for i in pu_test_data_preRes_LR_temp:\n",
    "            if i.features[-1] != 0 and i.features[-2] != 0 and i.features[-3] != 0:\n",
    "                predict_to_0_prob_LR.append(i.probability[0])\n",
    "        prob_LR_max = max(predict_to_0_prob_LR)\n",
    "        prob_LR_mean = sum(predict_to_0_prob_LR)/len(predict_to_0_prob_LR)\n",
    "        print(len(predict_to_0_prob_LR), prob_LR_max, min(predict_to_0_prob_LR), prob_LR_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_LR) ++ max(predict_to_0_prob_LR) ++ min(predict_to_0_prob_LR) ++ mean(predict_to_0_prob_LR) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_LR)) + ', ' + str(prob_LR_max) + ', ' + str(min(predict_to_0_prob_LR)) + ', ' + str(prob_LR_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        bbb_LR = pu_test_data_preRes_LR.filter(pu_test_data_preRes_LR.label == 0).select('features', 'probability').collect()\n",
    "        RN_LR_max = []\n",
    "        RN_LR_mean = []\n",
    "        for i in bbb_LR:\n",
    "            if i.probability[0] >= prob_LR_max:\n",
    "                RN_LR_max.append(i.features)\n",
    "            if i.probability[0] > prob_LR_mean:\n",
    "                RN_LR_mean.append(i.features)\n",
    "\n",
    "        log_file1.write('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n')\n",
    "        log_file1.write(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "        print('len(RN_LR_max): '+str(len(RN_LR_max))+'\\n)\n",
    "        print(\"len(RN_LR_mean):\" + str(len(RN_LR_mean))+'\\n')\n",
    "        sc.parallelize(RN_LR_max).saveAsTextFile(base_path + \"RN/RN_LR_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_LR_mean).saveAsTextFile(base_path + \"RN/RN_LR_mean-\" + str(alfa))\n",
    "        \n",
    "        ##############################\n",
    "        # NB\n",
    "        pubNB = NaiveBayes().fit(pu_train_data)\n",
    "        pu_test_data_preRes_NB = pubNB.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features')\n",
    "\n",
    "        pu_test_data_preRes_NB_temp = pu_test_data_preRes_NB.filter((col('prediction') == 0) & (col('label') == 0))\\\n",
    "        .select('features', 'probability').collect()\n",
    "\n",
    "        predict_to_0_prob_NB = []\n",
    "        for i in pu_test_data_preRes_NB_temp:\n",
    "            if i.features[-1] != 0 and i.features[-2] != 0 and i.features[-3] != 0:\n",
    "                predict_to_0_prob_NB.append(i.probability[0])\n",
    "        prob_NB_max = max(predict_to_0_prob_NB)\n",
    "        prob_NB_mean = sum(predict_to_0_prob_NB)/len(predict_to_0_prob_NB)\n",
    "        print(len(predict_to_0_prob_NB), prob_NB_max, min(predict_to_0_prob_NB), prob_NB_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_NB) ++ max(predict_to_0_prob_NB) ++ min(predict_to_0_prob_NB) ++ mean(predict_to_0_prob_NB) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_NB)) + ', ' + str(prob_NB_max) + ', ' + str(min(predict_to_0_prob_NB)) + ', ' + str(prob_NB_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        bbb_NB = pu_test_data_preRes_NB.filter(pu_test_data_preRes_NB.label == 0).select('features', 'probability').collect()\n",
    "        RN_NB_max = []\n",
    "        RN_NB_mean = []\n",
    "        for i in bbb_NB:\n",
    "            if i.probability[0] >= prob_NB_max:\n",
    "                RN_NB_max.append(i.features)\n",
    "            if i.probability[0] > prob_NB_mean:\n",
    "                RN_NB_mean.append(i.features)\n",
    "\n",
    "        log_file1.write(\"len(RN_NB_max): \"+str(len(RN_NB_max))+'\\n')\n",
    "        log_file1.write(\"len(RN_NB_mean): \"+str(len(RN_NB_mean))+'\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        print('len(RN_NB_max): '+str(len(RN_NB_max))+'\\n)\n",
    "        print(\"len(RN_NB_mean):\" + str(len(RN_NB_mean))+'\\n')\n",
    "        sc.parallelize(RN_NB_max).saveAsTextFile(base_path + \"RN/RN_NB_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_NB_mean).saveAsTextFile(base_path + \"RN/RN_NB_mean-\" + str(alfa))\n",
    "        \n",
    "        ###############################\n",
    "        # DT\n",
    "        pubDT = DecisionTreeClassifier().fit(pu_train_data)\n",
    "        pu_test_data_preRes_DT = pubDT.transform(pu_test_data.select(\"features\")).join(pu_test_data, 'features')\n",
    "\n",
    "        pu_test_data_preRes_DT_temp = pu_test_data_preRes_DT.filter((col('prediction') == 0) & (col('label') == 0))\\\n",
    "        .select('features', 'probability').collect()\n",
    "\n",
    "        predict_to_0_prob_DT = []\n",
    "        for i in pu_test_data_preRes_DT_temp:\n",
    "            if i.features[-1] != 0 and i.features[-2] != 0 and i.features[-3] != 0:\n",
    "                predict_to_0_prob_DT.append(i.probability[0])\n",
    "        prob_DT_max = max(predict_to_0_prob_DT)\n",
    "        prob_DT_mean = sum(predict_to_0_prob_DT)/len(predict_to_0_prob_DT)\n",
    "        print(len(predict_to_0_prob_DT), prob_DT_max, min(predict_to_0_prob_DT), prob_DT_mean)\n",
    "        log_file1.write(\"len(predict_to_0_prob_DT) ++ max(predict_to_0_prob_DT) ++ min(predict_to_0_prob_DT) ++ mean(predict_to_0_prob_DT) \\n\")\n",
    "        log_file1.write(str(len(predict_to_0_prob_DT)) + ', ' + str(prob_DT_max) + ', ' + str(min(predict_to_0_prob_DT)) + ', ' + str(prob_DT_mean) + '\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        \n",
    "        bbb_DT = pu_test_data_preRes_DT.filter(pu_test_data_preRes_DT.label == 0).select('features', 'probability').collect()\n",
    "        RN_DT_max = []\n",
    "        RN_DT_mean = []\n",
    "        for i in bbb_DT:\n",
    "            if i.probability[0] >= prob_DT_max:\n",
    "                RN_DT_max.append(i.features)\n",
    "            if i.probability[0] > prob_DT_mean:\n",
    "                RN_DT_mean.append(i.features)\n",
    "\n",
    "        log_file1.write(\"len(RN_DT_max): \"+str(len(RN_DT_max))+'\\n')\n",
    "        log_file1.write(\"len(RN_DT_max): \"+str(len(RN_DT_mean))+'\\n')\n",
    "        log_file1.write(time.asctime() + '\\n')\n",
    "        print('len(RN_DT_max): '+str(len(RN_DT_max))+'\\n)\n",
    "        print(\"len(RN_DT_mean):\" + str(len(RN_DT_mean))+'\\n')\n",
    "        sc.parallelize(RN_DT_max).saveAsTextFile(base_path + \"RN/RN_DT_max-\" + str(alfa))\n",
    "        sc.parallelize(RN_DT_mean).saveAsTextFile(base_path + \"RN/RN_DT_mean-\" + str(alfa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 15 18:40:15 2017\n",
      "Wed Feb 15 19:39:03 2017\n",
      "Wed Feb 15 20:37:47 2017\n",
      "Wed Feb 15 21:39:21 2017\n"
     ]
    }
   ],
   "source": [
    "with open('qq_PU_alfa_2.log', 'a+') as log_file2:\n",
    "    for alfa in [0.05]:# 0.10, 0.15, 0.20, 0.25, 0.30, 0.35]:\n",
    "        log_file2.write('\\n' + time.asctime() + '\\n')\n",
    "        log_file2.write(\"===Predict with RN===\\n\" + '\\n')\n",
    "        log_file2.write( \"==========alfa: \" + str(alfa) + \"============\\n\")\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # LR\n",
    "        RN_LR = sc.textFile(base_path + \"RN/RN_LR_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "        \n",
    "        PU_train_data_LR = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_LR.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()  ###### RN feature dense\n",
    "        \n",
    "        lr = LogisticRegression().fit(PU_train_data_LR)\n",
    "        predict_4th_day_LR = lr.transform(test_data_4thday.select('features')).join(test_data_4thday, 'features')\n",
    "        \n",
    "        y_label_PU_LR = [i.label for i in predict_4th_day_LR.select('label').collect()]\n",
    "        y_predict_PU_LR = [i.prediction for i in predict_4th_day_LR.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_LR = recall_score(y_label_PU_LR, y_predict_PU_LR)\n",
    "        precision_score_PU_LR = precision_score(y_label_PU_LR, y_predict_PU_LR)\n",
    "        f1_score_PU_LR = f1_score(y_label_PU_LR, y_predict_PU_LR)\n",
    "        \n",
    "        print(recall_score_PU_LR, precision_score_PU_LR, f1_score_PU_LR)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_LR: \" + str(recall_score_PU_LR) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_LR: \" + str(precision_score_PU_LR) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_LR: \" + str(f1_score_PU_LR) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # NB\n",
    "        RN_NB = sc.textFile(base_path + \"RN/RN_NB_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "        \n",
    "        PU_train_data_NB = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_NB.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()\n",
    "        \n",
    "        NB = NaiveBayes().fit(PU_train_data_NB)\n",
    "        predict_4th_day_NB = NB.transform(test_data_4thday.select('features')).join(test_data_4thday, 'features')\n",
    "        \n",
    "        y_label_PU_NB = [i.label for i in predict_4th_day_NB.select('label').collect()]\n",
    "        y_predict_PU_NB = [i.prediction for i in predict_4th_day_NB.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_NB = recall_score(y_label_PU_NB, y_predict_PU_NB)\n",
    "        precision_score_PU_NB = precision_score(y_label_PU_NB, y_predict_PU_NB)\n",
    "        f1_score_PU_NB = f1_score(y_label_PU_NB, y_predict_PU_NB)\n",
    "        \n",
    "        print(recall_score_PU_NB, precision_score_PU_NB, f1_score_PU_NB)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_NB: \" + str(recall_score_PU_NB) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_NB: \" + str(precision_score_PU_NB) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_NB: \" + str(f1_score_PU_NB) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        # DT\n",
    "        RN_DT = sc.textFile(base_path + \"RN/RN_DT_mean-\" + str(alfa)).map(lambda x: eval(x))\n",
    "        \n",
    "        PU_train_data_DT = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(RN_DT.map(lambda x: Row(label=0.0, features=Vectors.dense(x)))).toDF()\n",
    "        \n",
    "        DT = NaiveBayes().fit(PU_train_data_DT)\n",
    "        predict_4th_day_DT = DT.transform(test_data_4thday.select('features')).join(test_data_4thday, 'features')\n",
    "        \n",
    "        y_label_PU_DT = [i.label for i in predict_4th_day_DT.select('label').collect()]\n",
    "        y_predict_PU_DT = [i.prediction for i in predict_4th_day_DT.select('prediction').collect()]\n",
    "        \n",
    "        recall_score_PU_DT = recall_score(y_label_PU_DT, y_predict_PU_DT)\n",
    "        precision_score_PU_DT = precision_score(y_label_PU_DT, y_predict_PU_DT)\n",
    "        f1_score_PU_DT = f1_score(y_label_PU_DT, y_predict_PU_DT)\n",
    "        \n",
    "        print(recall_score_PU_DT, precision_score_PU_DT, f1_score_PU_DT)\n",
    "\n",
    "        log_file2.write('\\n')\n",
    "        log_file2.write(\"recall_score_PU_DT: \" + str(recall_score_PU_DT) + '\\n')\n",
    "        log_file2.write(\"precision_score_PU_DT: \" + str(precision_score_PU_DT) + '\\n')\n",
    "        log_file2.write(\"f1_score_PU_DT: \" + str(f1_score_PU_DT) + '\\n')\n",
    "        log_file2.write(time.asctime() + '\\n')\n",
    "        log_file2.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_4th_day_LR_xx = lr.transform(test_data_4thday.select('features')).join(test_data_4thday, 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_label_xx = [i.label for i in predict_4th_day_LR_xx.select('label').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict_xx = [i.prediction for i in predict_4th_day_LR_xx.select('prediction').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_score_PU_LR_xx = recall_score(y_label_xx, y_predict_xx)\n",
    "precision_score_PU_LR_xx = precision_score(y_label_xx, y_predict_xx)\n",
    "f1_score_PU_LR_xx = f1_score(y_label_xx, y_predict_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(recall_score_PU_LR_xx, precision_score_PU_LR_xx, f1_score_PU_LR_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xxx = [Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 27.0, 35.0, 29.0, 5.0, 3.0, 12.0, 5.0, 5.0, 1.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 2.0, 143.0, 740.0, 854.0, -7375.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 83.0, 123.0, 18094.0]), Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 6.0, 8.0, 2.0, 1.0, 0.0, 1.0, 23.0, 115.0, 114.0, -2767.0, 291.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 577.0, 0.0, 716.0, 0.0, 0.0, 0.0, 84.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 276.0, 0.0, 0.0, 0.0, 0.0, 0.0, 300.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.0, 0.0, 716.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 21.0, 15.0, 9.0, 8.0, 0.0, 14.0, 7.0, 23.0, 56.0, 145.0, 70.0, 92.0, 86.0, 80.0, 45.0, 31.0, 51.0, 53.0, 75.0, 126.0, 100.0, 75.0, 90.0, 99.0, 1378.0, 13684.0, 15854.0, 9779725.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize(xxx).saveAsTextFile(base_path + \"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = sc.textFile(base_path + \"xxx\").map(lambda x: Vectors.dense(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 27.0, 35.0, 29.0, 5.0, 3.0, 12.0, 5.0, 5.0, 1.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 2.0, 143.0, 740.0, 854.0, -7375.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 83.0, 123.0, 18094.0]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 6.0, 8.0, 2.0, 1.0, 0.0, 1.0, 23.0, 115.0, 114.0, -2767.0, 291.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 577.0, 0.0, 716.0, 0.0, 0.0, 0.0, 84.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 276.0, 0.0, 0.0, 0.0, 0.0, 0.0, 300.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.0, 0.0, 716.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 21.0, 15.0, 9.0, 8.0, 0.0, 14.0, 7.0, 23.0, 56.0, 145.0, 70.0, 92.0, 86.0, 80.0, 45.0, 31.0, 51.0, 53.0, 75.0, 126.0, 100.0, 75.0, 90.0, 99.0, 1378.0, 13684.0, 15854.0, 9779725.0])]\n"
     ]
    }
   ],
   "source": [
    "print(xx.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [('aa',[0,1,2,3,0]), ('aa',[0,1,2,4,0]), ('aasas',[10,1,1,0,0])]\n",
    "c = [('abs',[0,0,0,0,0]), ('aas',[0,1,1,0,0]), ('aaas',[110,1111,1,0,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = sc.parallelize(a).map(lambda x: Row(label=0.0, features=Vectors.dense(x[1]))).union((sc.parallelize(c).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1]))))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,1.0,2.0,3.0,...|  0.0|\n",
      "|[0.0,1.0,2.0,4.0,...|  0.0|\n",
      "|[10.0,1.0,1.0,0.0...|  0.0|\n",
      "|[0.0,0.0,0.0,0.0,...|  1.0|\n",
      "|[0.0,1.0,1.0,0.0,...|  1.0|\n",
      "|[110.0,1111.0,1.0...|  1.0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = [i.label for i in b.select('label').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0, 2.0, 3.0, 0.0]),\n",
       " DenseVector([0.0, 1.0, 2.0, 4.0, 0.0]),\n",
       " DenseVector([10.0, 1.0, 1.0, 0.0, 0.0]),\n",
       " DenseVector([0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " DenseVector([0.0, 1.0, 1.0, 0.0, 0.0]),\n",
       " DenseVector([110.0, 1111.0, 1.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxsx =[i.features for i in b.select('features').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
