{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/media/jw/5dccc50e-7c13-4186-bf6b-d894f02410be/wbb/result/qq_prediction/\"\n",
    "\n",
    "train_positive_pairs = sc.textFile(base_path + \"new_positive_pairs_features/day*/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "train_unlabled_pairs = sc.textFile(base_path + \"new_unlabled_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "\n",
    "test_positive_pairs = sc.textFile(base_path + \"test_positive_pairs/part-*\")\n",
    "\n",
    "# test_pairs_features = sc.textFile(base_path + \"new_test_pairs_features/part-*\").map(lambda x: (eval(x)[0], [abs(i) for y in eval(x)[1] for i in y ]))\n",
    "# test_pairs_features_50w = sc.textFile(base_path + \"test_pairs_features_50w/part-*\").map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_positive_pairs_list = test_positive_pairs.collect()   # 第四天数据中的正例对\n",
    "test_data_4thday = test_pairs_features_50w.map(lambda x: Row(label=1.0 if x[0] in test_positive_pairs_list else 0.0,\\\n",
    "                                                         features=Vectors.dense(x[1]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_label_noPU = [i.label for i in test_data_4thday.select('label').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pos_features = sc.textFile(base_path + \"test_pos_features/part-*\").map(lambda x: eval(x))\n",
    "test_neg_features = sc.textFile(base_path + \"test_neg_features_8w/part-*\").map(lambda x: eval(x))\n",
    "test_data_4thday = test_pos_features.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(test_neg_features.map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_label_noPU = [i.label for i in test_data_4thday.select('label').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157653\n"
     ]
    }
   ],
   "source": [
    "test_data_4thday_length = len(y_label_noPU)\n",
    "print(test_data_4thday_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 25 21:22:35 2017\n",
      "alfa: 0.05\n",
      "Sat Feb 25 21:22:50 2017\n",
      "Sat Feb 25 21:22:51 2017\n",
      "0.99475873437 0.528589806755 0.690346710517 0.5604460428916671\n",
      "Sat Feb 25 21:26:10 2017\n",
      "0.773737009517 0.435867302154 0.557614849188 0.39529219234648244\n",
      "Sat Feb 25 21:27:53 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.974501950987 0.565534198254 0.715715900332 0.61868787780759\n",
      "Sat Feb 25 21:29:11 2017\n",
      "0.95544280324 0.827179075523 0.886696504867 0.8797295325810482\n",
      "Sat Feb 25 21:32:29 2017\n",
      "Sat Feb 25 21:32:29 2017\n",
      "alfa: 0.1\n",
      "Sat Feb 25 21:32:55 2017\n",
      "Sat Feb 25 21:32:55 2017\n",
      "0.995222335261 0.509124207808 0.673637049079 0.5250137961218626\n",
      "Sat Feb 25 21:37:36 2017\n",
      "0.772925707957 0.435656787811 0.5572318519 0.3949877262088257\n",
      "Sat Feb 25 21:40:03 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.971295378157 0.57413412499 0.721681346072 0.6309933842045505\n",
      "Sat Feb 25 21:41:45 2017\n",
      "0.928219128688 0.843522527794 0.883846403806 0.8798310212936005\n",
      "Sat Feb 25 21:46:29 2017\n",
      "Sat Feb 25 21:46:29 2017\n",
      "alfa: 0.15\n",
      "Sat Feb 25 21:47:01 2017\n",
      "Sat Feb 25 21:47:01 2017\n",
      "0.988178177276 0.524096056388 0.684929061397 0.5522000849967967\n",
      "Sat Feb 25 21:53:08 2017\n",
      "0.77288707455 0.435644498643 0.557211759408 0.39496869707522214\n",
      "Sat Feb 25 21:56:16 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.957735052091 0.584470902589 0.725931927105 0.6437999911197376\n",
      "Sat Feb 25 21:58:22 2017\n",
      "0.857623015209 0.904715327872 0.880539983076 0.8853811852613017\n",
      "Sat Feb 25 22:04:29 2017\n",
      "Sat Feb 25 22:04:29 2017\n",
      "alfa: 0.2\n",
      "Sat Feb 25 22:05:20 2017\n",
      "Sat Feb 25 22:05:20 2017\n",
      "0.988925089823 0.514660447286 0.676995909444 0.5351943825997603\n",
      "Sat Feb 25 22:13:05 2017\n",
      "0.77299009697 0.435674106333 0.55726275136 0.3950130983869638\n",
      "Sat Feb 25 22:17:01 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.960787091291 0.573784108038 0.718486525007 0.629153901289541\n",
      "Sat Feb 25 22:19:46 2017\n",
      "0.793890770479 0.934612877306 0.858523542273 0.8711220211477104\n",
      "Sat Feb 25 22:27:35 2017\n",
      "Sat Feb 25 22:27:35 2017\n",
      "alfa: 0.25\n",
      "Sat Feb 25 22:28:31 2017\n",
      "Sat Feb 25 22:28:31 2017\n",
      "0.989530346542 0.553028558268 0.709520445805 0.6009146670218771\n",
      "Sat Feb 25 22:37:30 2017\n",
      "0.772526496079 0.435536098043 0.557029374759 0.39480377791732474\n",
      "Sat Feb 25 22:42:04 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.953111920982 0.587924090653 0.727247715437 0.6478595396218277\n",
      "Sat Feb 25 22:45:09 2017\n",
      "0.780047132757 0.939830258646 0.852516466813 0.8670624726456204\n",
      "Sat Feb 25 22:54:11 2017\n",
      "Sat Feb 25 22:54:11 2017\n",
      "alfa: 0.3\n",
      "Sat Feb 25 22:55:14 2017\n",
      "Sat Feb 25 22:55:14 2017\n",
      "0.986658596577 0.611302599454 0.754895633711 0.6844145052742415\n",
      "Sat Feb 25 23:05:43 2017\n",
      "0.77217879541 0.435438074144 0.556858811839 0.3946578878930309\n",
      "Sat Feb 25 23:11:01 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.957799441103 0.571055642146 0.715511601955 0.6248469740506049\n",
      "Sat Feb 25 23:14:32 2017\n",
      "0.80732231852 0.931570970043 0.865007692361 0.8758856475931317\n",
      "Sat Feb 25 23:25:05 2017\n",
      "Sat Feb 25 23:25:05 2017\n",
      "alfa: 0.35\n",
      "Sat Feb 25 23:26:26 2017\n",
      "Sat Feb 25 23:26:26 2017\n",
      "0.982717989002 0.576001630386 0.726297605848 0.6351797935973309\n",
      "Sat Feb 25 23:38:25 2017\n",
      "0.772359084646 0.435492303224 0.55695003552 0.3947403474719796\n",
      "Sat Feb 25 23:44:30 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.95020153761 0.581981953559 0.721845849822 0.6393027725447661\n",
      "Sat Feb 25 23:48:33 2017\n",
      "0.765958816788 0.932652805218 0.841126517567 0.8574781323539673\n",
      "Sun Feb 26 00:00:40 2017\n",
      "Sun Feb 26 00:00:40 2017\n",
      "alfa: 0.4\n",
      "Sun Feb 26 00:02:07 2017\n",
      "Sun Feb 26 00:02:07 2017\n",
      "0.991462016921 0.53443009857 0.694501450073 0.5703665645436496\n",
      "Sun Feb 26 00:15:25 2017\n",
      "0.772462107066 0.435518768605 0.556998463203 0.3947784057391867\n",
      "Sun Feb 26 00:22:10 2017\n",
      "#### LogisticRegression ####\n",
      "\n",
      "0.949673547706 0.581365098385 0.721218967144 0.6383766880427267\n",
      "Sun Feb 26 00:26:38 2017\n",
      "0.771715194519 0.929546441645 0.843309574236 0.8587467412608704\n",
      "Sun Feb 26 00:40:03 2017\n"
     ]
    }
   ],
   "source": [
    "for alfa in [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40]: #, 0.20]:#, 0.30]:#, 0.35, 0.40]: # 0.05, , 0.25\n",
    "    with open('qq_noPU_alfa_v3331.log', 'a+') as log_file:\n",
    "        log_file.write('===========predict whitout PU=========== \\n')\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        log_file.write( \"++++++++++alfa: \" + str(alfa) + \"+++++++++++++\\n\")\n",
    "        print(time.asctime())\n",
    "        print(\"alfa: \" + str(alfa))\n",
    "        # train_unlabeled -> train_neg\n",
    "        #train_negative_pairs = sc.parallelize(train_unlabled_pairs.takeSample(False, int(alfa * 8268837)))\n",
    "        #train_negative_pairs.coalesce(20).saveAsTextFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa))\n",
    "        train_negative_pairs = sc.textFile(base_path + \"wbb_train_negative_pairs-\" + str(alfa)).map(lambda x: eval(x))\n",
    "\n",
    "        log_file.write(\"length of train_negative_pairs: \" + str(train_negative_pairs.count()) + '\\n')\n",
    "        print(time.asctime())\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "\n",
    "        #============ without PU =======================\n",
    "        # predict without PU\n",
    "        train_data_noPU = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(train_negative_pairs.map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\n",
    "        \n",
    "        \"\"\"train_data_noPU = sc.union([train_positive_pairs]*int(alfa/0.015)).map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union((train_negative_pairs.map(lambda x: Row(label=0.0, features=Vectors.dense(x[1]))))).toDF()\"\"\"\n",
    "\n",
    "        \"\"\"train_data_noPU = train_positive_pairs.map(lambda x: Row(label=1.0, features=Vectors.dense(x[1])))\\\n",
    "        .union(sc.parallelize(train_negative_pairs.takeSample(False, int(100000)))\\\n",
    "               .map(lambda x: Row(label=0.0, features=Vectors.dense(x[1])))).toDF()\"\"\"\n",
    "\n",
    "        #y_label_noPU = [i.label for i in test_data_4thday.select('label').collect()]\n",
    "\n",
    "        print(time.asctime())\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        \n",
    "        ### DecisionTree\n",
    "        log_file.write(\"#### DecisionTree ####\\n\")\n",
    "        DT_noPU = DecisionTreeClassifier().fit(train_data_noPU)\n",
    "        DT_predict_4th_day_noPU = DT_noPU.transform(test_data_4thday.select('features'))\n",
    "\n",
    "        y_predict_noPU_DT = [i.prediction for i in DT_predict_4th_day_noPU.select('prediction').collect()]\n",
    "\n",
    "        recall_score_noPU_DT = recall_score(y_label_noPU, y_predict_noPU_DT)\n",
    "        precision_score_noPU_DT = precision_score(y_label_noPU, y_predict_noPU_DT)\n",
    "        f1_score_noPU_DT = f1_score(y_label_noPU, y_predict_noPU_DT)\n",
    "        tp_DT = 0\n",
    "        tn_DT = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label_noPU[i] == 1.0 and y_predict_noPU_DT[i] == 1.0:\n",
    "                tp_DT += 1\n",
    "            if y_label_noPU[i] == 0.0 and y_predict_noPU_DT[i] == 0.0:\n",
    "                tn_DT += 1\n",
    "        accuracy_DT = (tp_DT + tn_DT)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_noPU_DT, precision_score_noPU_DT, f1_score_noPU_DT, accuracy_DT)\n",
    "\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"recall_score_noPU_DT: \" + str(recall_score_noPU_DT) + '\\n')\n",
    "        log_file.write(\"precision_score_noPU_DT: \" + str(precision_score_noPU_DT) + '\\n')\n",
    "        log_file.write(\"f1_score_noPU_DT: \" + str(f1_score_noPU_DT) + '\\n')\n",
    "        log_file.write(\"accuracy_DT: \" + str(accuracy_DT) + '\\n')\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        log_file.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        ### NB\n",
    "        log_file.write(\"#### NaiveBayes ####\\n\")\n",
    "        NB_noPU = NaiveBayes().fit(train_data_noPU)\n",
    "        NB_predict_4th_day_noPU = NB_noPU.transform(test_data_4thday.select('features'))\n",
    "\n",
    "        y_predict_noPU_NB = [i.prediction for i in NB_predict_4th_day_noPU.select('prediction').collect()]\n",
    "\n",
    "        recall_score_noPU_NB = recall_score(y_label_noPU, y_predict_noPU_NB)\n",
    "        precision_score_noPU_NB = precision_score(y_label_noPU, y_predict_noPU_NB)\n",
    "        f1_score_noPU_NB = f1_score(y_label_noPU, y_predict_noPU_NB)\n",
    "        tp_NB = 0\n",
    "        tn_NB = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label_noPU[i] == 1.0 and y_predict_noPU_NB[i] == 1.0:\n",
    "                tp_NB += 1\n",
    "            if y_label_noPU[i] == 0.0 and y_predict_noPU_NB[i] == 0.0:\n",
    "                tn_NB += 1\n",
    "        accuracy_NB = (tp_NB + tn_NB)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_noPU_NB, precision_score_noPU_NB, f1_score_noPU_NB, accuracy_NB)\n",
    "\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"recall_score_noPU_NB: \" + str(recall_score_noPU_NB) + '\\n')\n",
    "        log_file.write(\"precision_score_noPU_NB: \" + str(precision_score_noPU_NB) + '\\n')\n",
    "        log_file.write(\"f1_score_noPU_NB: \" + str(f1_score_noPU_NB) + '\\n')\n",
    "        log_file.write(\"accuracy_NB: \" + str(accuracy_NB) + '\\n')\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        log_file.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        ### LR\n",
    "        log_file.write(\"#### LogisticRegression ####\\n\")\n",
    "        print(\"#### LogisticRegression ####\\n\")\n",
    "        LR_noPU = LogisticRegression().fit(train_data_noPU)\n",
    "        \n",
    "        LR_predict_4th_day_noPU = LR_noPU.transform(test_data_4thday.select('features'))\n",
    "\n",
    "        y_predict_noPU_LR = [i.prediction for i in LR_predict_4th_day_noPU.select('prediction').collect()]\n",
    "\n",
    "        recall_score_noPU_LR = recall_score(y_label_noPU, y_predict_noPU_LR)\n",
    "        precision_score_noPU_LR = precision_score(y_label_noPU, y_predict_noPU_LR)\n",
    "        f1_score_noPU_LR = f1_score(y_label_noPU, y_predict_noPU_LR)\n",
    "        tp_LR = 0\n",
    "        tn_LR = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label_noPU[i] == 1.0 and y_predict_noPU_LR[i] == 1.0:\n",
    "                tp_LR += 1\n",
    "            if y_label_noPU[i] == 0.0 and y_predict_noPU_LR[i] == 0.0:\n",
    "                tn_LR += 1\n",
    "        accuracy_LR = (tp_LR + tn_LR)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_noPU_LR, precision_score_noPU_LR, f1_score_noPU_LR, accuracy_LR)\n",
    "\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"recall_score_noPU_LR: \" + str(recall_score_noPU_LR) + '\\n')\n",
    "        log_file.write(\"precision_score_noPU_LR: \" + str(precision_score_noPU_LR) + '\\n')\n",
    "        log_file.write(\"f1_score_noPU_LR: \" + str(f1_score_noPU_LR) + '\\n')\n",
    "        log_file.write(\"accuracy_LR: \" + str(accuracy_LR) + '\\n')\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        log_file.write('\\n')\n",
    "\n",
    "        print(time.asctime())\n",
    "        \n",
    "        ## RandomForestClassifier\n",
    "        log_file.write(\"#### RandomForestClassifier ####\\n\")\n",
    "        RF_noPU = RandomForestClassifier().fit(train_data_noPU)\n",
    "        \n",
    "        RF_predict_4th_day_noPU = RF_noPU.transform(test_data_4thday.select('features'))\n",
    "\n",
    "        y_predict_noPU_RF = [i.prediction for i in RF_predict_4th_day_noPU.select('prediction').collect()]\n",
    "\n",
    "        recall_score_noPU_RF = recall_score(y_label_noPU, y_predict_noPU_RF)\n",
    "        precision_score_noPU_RF = precision_score(y_label_noPU, y_predict_noPU_RF)\n",
    "        f1_score_noPU_RF = f1_score(y_label_noPU, y_predict_noPU_RF)\n",
    "        tp_RF = 0\n",
    "        tn_RF = 0\n",
    "        for i in range(test_data_4thday_length):\n",
    "            if y_label_noPU[i] == 1.0 and y_predict_noPU_RF[i] == 1.0:\n",
    "                tp_RF += 1\n",
    "            if y_label_noPU[i] == 0.0 and y_predict_noPU_RF[i] == 0.0:\n",
    "                tn_RF += 1\n",
    "        accuracy_RF = (tp_RF + tn_RF)/test_data_4thday_length\n",
    "        \n",
    "        print(recall_score_noPU_RF, precision_score_noPU_RF, f1_score_noPU_RF, accuracy_RF)\n",
    "\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"recall_score_noPU_RF: \" + str(recall_score_noPU_RF) + '\\n')\n",
    "        log_file.write(\"precision_score_noPU_RF: \" + str(precision_score_noPU_RF) + '\\n')\n",
    "        log_file.write(\"f1_score_noPU_RF: \" + str(f1_score_noPU_RF) + '\\n')\n",
    "        log_file.write(\"accuracy_RF: \" + str(accuracy_RF) + '\\n')\n",
    "        log_file.write(time.asctime() + '\\n')\n",
    "        log_file.write('\\n')\n",
    "\n",
    "        print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "rdd3 = sc.parallelize([7, 8, 9])\n",
    "\n",
    "rdd = sc.union([rdd1]*int(0.2/0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
